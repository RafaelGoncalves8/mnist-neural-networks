{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.datasets as datasets\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "torch.random.seed = 42\n",
    "\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the MNIST dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_trainset = datasets.MNIST(root='../data', train=True, download=True,\n",
    "                                transform=None)\n",
    "\n",
    "mnist_testset = datasets.MNIST(root='../data', train=False, download=True,\n",
    "                               transform=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset MNIST\n",
      "    Number of datapoints: 60000\n",
      "    Split: train\n",
      "    Root Location: ../data\n",
      "    Transforms (if any): None\n",
      "    Target Transforms (if any): None\n",
      "\n",
      "Dataset MNIST\n",
      "    Number of datapoints: 10000\n",
      "    Split: test\n",
      "    Root Location: ../data\n",
      "    Transforms (if any): None\n",
      "    Target Transforms (if any): None\n"
     ]
    }
   ],
   "source": [
    "print(mnist_trainset)\n",
    "print('')\n",
    "print(mnist_testset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([28, 28])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist_trainset.data[0].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "size_len = mnist_trainset.data[0].size()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 5 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure(1)\n",
    "for i, img in enumerate(mnist_trainset.data[0:5]):\n",
    "    ax = fig.add_subplot(1,5,i+1)\n",
    "    ax.set_axis_off()\n",
    "    ax = plt.imshow(img)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Net architecture and train/test routines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    \"\"\"MLP with 3 ReLU hidden layers and 1 softmax output layer\"\"\"\n",
    "    \n",
    "    def __init__(self, H, C):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(size_len*size_len, H)\n",
    "        self.fc2 = nn.Linear(H, H)\n",
    "        self.fc3 = nn.Linear(H, H)\n",
    "        self.fc4 = nn.Linear(H, C)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, size_len*size_len)\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.relu(self.fc3(x))\n",
    "        x = self.softmax(self.fc4(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, x_train, y_train, optimizer, criterion, epoch, disp=''):\n",
    "    model.train()\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    output = model(x_train)\n",
    "    loss = criterion(output, y_train)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if disp=='print':\n",
    "        print(\"Train Epoch: {}\\tLoss: {:.6f}\".format(epoch, loss.item()))\n",
    "    elif disp=='graph':\n",
    "        pass\n",
    "    \n",
    "    return loss\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, x_test, y_test, criterion, disp=''):\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = model(x_test)\n",
    "        test_loss = criterion(output, y_test)\n",
    "\n",
    "    if disp=='print':\n",
    "        print(\"\\nTest set: Average loss: {:.4f}\\n\".format(test_loss))\n",
    "    elif disp=='graph':\n",
    "        pass\n",
    "        \n",
    "    return test_loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = mnist_trainset.data.float()\n",
    "y_train = mnist_trainset.targets\n",
    "\n",
    "X_test = mnist_testset.data.float()\n",
    "y_test = mnist_testset.targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (fc1): Linear(in_features=784, out_features=100, bias=True)\n",
      "  (fc2): Linear(in_features=100, out_features=100, bias=True)\n",
      "  (fc3): Linear(in_features=100, out_features=100, bias=True)\n",
      "  (fc4): Linear(in_features=100, out_features=10, bias=True)\n",
      "  (relu): ReLU()\n",
      "  (softmax): LogSoftmax()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = Net(100, 10)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.01\n",
    "gamma = 10\n",
    "max_epoch = 100\n",
    "optimizer = optim.SGD(model.parameters(), lr=alpha)\n",
    "#optim.Adam()\n",
    "criterion = torch.nn.NLLLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch = count = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0\tLoss: 5.291198\n",
      "Train Epoch: 1\tLoss: 4.505117\n",
      "Train Epoch: 2\tLoss: 5.022614\n",
      "Train Epoch: 3\tLoss: 2.534461\n",
      "Train Epoch: 4\tLoss: 2.140278\n",
      "Train Epoch: 5\tLoss: 1.884296\n",
      "Train Epoch: 6\tLoss: 1.600400\n",
      "Train Epoch: 7\tLoss: 1.346500\n",
      "Train Epoch: 8\tLoss: 1.224009\n",
      "Train Epoch: 9\tLoss: 1.132581\n",
      "Train Epoch: 10\tLoss: 1.073515\n",
      "Train Epoch: 11\tLoss: 1.050495\n",
      "Train Epoch: 12\tLoss: 1.096614\n",
      "Train Epoch: 13\tLoss: 1.126030\n",
      "Train Epoch: 14\tLoss: 1.124170\n",
      "Train Epoch: 15\tLoss: 0.942829\n",
      "Train Epoch: 16\tLoss: 0.825358\n",
      "Train Epoch: 17\tLoss: 0.772896\n",
      "Train Epoch: 18\tLoss: 0.737629\n",
      "Train Epoch: 19\tLoss: 0.735390\n",
      "Train Epoch: 20\tLoss: 0.737501\n",
      "Train Epoch: 21\tLoss: 0.795219\n",
      "Train Epoch: 22\tLoss: 0.811436\n",
      "Train Epoch: 23\tLoss: 0.824166\n",
      "Train Epoch: 24\tLoss: 0.767483\n",
      "Train Epoch: 25\tLoss: 0.682649\n",
      "Train Epoch: 26\tLoss: 0.615025\n",
      "Train Epoch: 27\tLoss: 0.581139\n",
      "Train Epoch: 28\tLoss: 0.554944\n",
      "Train Epoch: 29\tLoss: 0.540122\n",
      "Train Epoch: 30\tLoss: 0.526429\n",
      "Train Epoch: 31\tLoss: 0.519906\n",
      "Train Epoch: 32\tLoss: 0.512550\n",
      "Train Epoch: 33\tLoss: 0.514648\n",
      "Train Epoch: 34\tLoss: 0.511886\n",
      "Train Epoch: 35\tLoss: 0.523395\n",
      "Train Epoch: 36\tLoss: 0.514939\n",
      "Train Epoch: 37\tLoss: 0.522633\n",
      "Train Epoch: 38\tLoss: 0.501883\n",
      "Train Epoch: 39\tLoss: 0.495223\n",
      "Train Epoch: 40\tLoss: 0.477066\n",
      "Train Epoch: 41\tLoss: 0.463698\n",
      "Train Epoch: 42\tLoss: 0.451989\n",
      "Train Epoch: 43\tLoss: 0.439244\n",
      "Train Epoch: 44\tLoss: 0.432314\n",
      "Train Epoch: 45\tLoss: 0.422208\n",
      "Train Epoch: 46\tLoss: 0.417912\n",
      "Train Epoch: 47\tLoss: 0.410170\n",
      "Train Epoch: 48\tLoss: 0.407266\n",
      "Train Epoch: 49\tLoss: 0.401061\n",
      "Train Epoch: 50\tLoss: 0.398915\n",
      "Train Epoch: 51\tLoss: 0.393485\n",
      "Train Epoch: 52\tLoss: 0.391795\n",
      "Train Epoch: 53\tLoss: 0.386902\n",
      "Train Epoch: 54\tLoss: 0.385414\n",
      "Train Epoch: 55\tLoss: 0.380684\n",
      "Train Epoch: 56\tLoss: 0.379146\n",
      "Train Epoch: 57\tLoss: 0.374362\n",
      "Train Epoch: 58\tLoss: 0.372506\n",
      "Train Epoch: 59\tLoss: 0.367698\n",
      "Train Epoch: 60\tLoss: 0.365461\n",
      "Train Epoch: 61\tLoss: 0.360610\n",
      "Train Epoch: 62\tLoss: 0.357989\n",
      "Train Epoch: 63\tLoss: 0.353233\n",
      "Train Epoch: 64\tLoss: 0.350413\n",
      "Train Epoch: 65\tLoss: 0.345842\n",
      "Train Epoch: 66\tLoss: 0.343062\n",
      "Train Epoch: 67\tLoss: 0.338883\n",
      "Train Epoch: 68\tLoss: 0.336368\n",
      "Train Epoch: 69\tLoss: 0.332570\n",
      "Train Epoch: 70\tLoss: 0.330259\n",
      "Train Epoch: 71\tLoss: 0.326799\n",
      "Train Epoch: 72\tLoss: 0.324639\n",
      "Train Epoch: 73\tLoss: 0.321503\n",
      "Train Epoch: 74\tLoss: 0.319540\n",
      "Train Epoch: 75\tLoss: 0.316646\n",
      "Train Epoch: 76\tLoss: 0.314876\n",
      "Train Epoch: 77\tLoss: 0.312274\n",
      "Train Epoch: 78\tLoss: 0.310700\n",
      "Train Epoch: 79\tLoss: 0.308298\n",
      "Train Epoch: 80\tLoss: 0.306913\n",
      "Train Epoch: 81\tLoss: 0.304657\n",
      "Train Epoch: 82\tLoss: 0.303439\n",
      "Train Epoch: 83\tLoss: 0.301375\n",
      "Train Epoch: 84\tLoss: 0.300336\n",
      "Train Epoch: 85\tLoss: 0.298369\n",
      "Train Epoch: 86\tLoss: 0.297504\n",
      "Train Epoch: 87\tLoss: 0.295580\n",
      "Train Epoch: 88\tLoss: 0.294852\n",
      "Train Epoch: 89\tLoss: 0.292968\n",
      "Train Epoch: 90\tLoss: 0.292370\n",
      "Train Epoch: 91\tLoss: 0.290501\n",
      "Train Epoch: 92\tLoss: 0.289963\n",
      "Train Epoch: 93\tLoss: 0.288053\n",
      "Train Epoch: 94\tLoss: 0.287554\n",
      "Train Epoch: 95\tLoss: 0.285626\n",
      "Train Epoch: 96\tLoss: 0.285085\n",
      "Train Epoch: 97\tLoss: 0.283059\n",
      "Train Epoch: 98\tLoss: 0.282390\n",
      "Train Epoch: 99\tLoss: 0.280224\n",
      "Train Epoch: 100\tLoss: 0.279349\n",
      "Train Epoch: 101\tLoss: 0.277134\n",
      "Train Epoch: 102\tLoss: 0.276063\n",
      "Train Epoch: 103\tLoss: 0.273837\n",
      "Train Epoch: 104\tLoss: 0.272603\n",
      "Train Epoch: 105\tLoss: 0.270466\n",
      "Train Epoch: 106\tLoss: 0.269167\n",
      "Train Epoch: 107\tLoss: 0.267155\n",
      "Train Epoch: 108\tLoss: 0.265835\n",
      "Train Epoch: 109\tLoss: 0.263941\n",
      "Train Epoch: 110\tLoss: 0.262625\n",
      "Train Epoch: 111\tLoss: 0.260904\n",
      "Train Epoch: 112\tLoss: 0.259617\n",
      "Train Epoch: 113\tLoss: 0.258028\n",
      "Train Epoch: 114\tLoss: 0.256811\n",
      "Train Epoch: 115\tLoss: 0.255351\n",
      "Train Epoch: 116\tLoss: 0.254196\n",
      "Train Epoch: 117\tLoss: 0.252850\n",
      "Train Epoch: 118\tLoss: 0.251746\n",
      "Train Epoch: 119\tLoss: 0.250500\n",
      "Train Epoch: 120\tLoss: 0.249428\n",
      "Train Epoch: 121\tLoss: 0.248261\n",
      "Train Epoch: 122\tLoss: 0.247242\n",
      "Train Epoch: 123\tLoss: 0.246130\n",
      "Train Epoch: 124\tLoss: 0.245156\n",
      "Train Epoch: 125\tLoss: 0.244095\n",
      "Train Epoch: 126\tLoss: 0.243166\n",
      "Train Epoch: 127\tLoss: 0.242156\n",
      "Train Epoch: 128\tLoss: 0.241257\n",
      "Train Epoch: 129\tLoss: 0.240283\n",
      "Train Epoch: 130\tLoss: 0.239395\n",
      "Train Epoch: 131\tLoss: 0.238458\n",
      "Train Epoch: 132\tLoss: 0.237612\n",
      "Train Epoch: 133\tLoss: 0.236716\n",
      "Train Epoch: 134\tLoss: 0.235898\n",
      "Train Epoch: 135\tLoss: 0.235002\n",
      "Train Epoch: 136\tLoss: 0.234215\n",
      "Train Epoch: 137\tLoss: 0.233342\n",
      "Train Epoch: 138\tLoss: 0.232587\n",
      "Train Epoch: 139\tLoss: 0.231727\n",
      "Train Epoch: 140\tLoss: 0.230975\n",
      "Train Epoch: 141\tLoss: 0.230133\n",
      "Train Epoch: 142\tLoss: 0.229397\n",
      "Train Epoch: 143\tLoss: 0.228586\n",
      "Train Epoch: 144\tLoss: 0.227876\n",
      "Train Epoch: 145\tLoss: 0.227085\n",
      "Train Epoch: 146\tLoss: 0.226406\n",
      "Train Epoch: 147\tLoss: 0.225635\n",
      "Train Epoch: 148\tLoss: 0.224996\n",
      "Train Epoch: 149\tLoss: 0.224242\n",
      "Train Epoch: 150\tLoss: 0.223629\n",
      "Train Epoch: 151\tLoss: 0.222900\n",
      "Train Epoch: 152\tLoss: 0.222334\n",
      "Train Epoch: 153\tLoss: 0.221628\n",
      "Train Epoch: 154\tLoss: 0.221084\n",
      "Train Epoch: 155\tLoss: 0.220407\n",
      "Train Epoch: 156\tLoss: 0.219875\n",
      "Train Epoch: 157\tLoss: 0.219199\n",
      "Train Epoch: 158\tLoss: 0.218681\n",
      "Train Epoch: 159\tLoss: 0.218008\n",
      "Train Epoch: 160\tLoss: 0.217481\n",
      "Train Epoch: 161\tLoss: 0.216795\n",
      "Train Epoch: 162\tLoss: 0.216275\n",
      "Train Epoch: 163\tLoss: 0.215584\n",
      "Train Epoch: 164\tLoss: 0.215076\n",
      "Train Epoch: 165\tLoss: 0.214390\n",
      "Train Epoch: 166\tLoss: 0.213882\n",
      "Train Epoch: 167\tLoss: 0.213187\n",
      "Train Epoch: 168\tLoss: 0.212679\n",
      "Train Epoch: 169\tLoss: 0.211972\n",
      "Train Epoch: 170\tLoss: 0.211466\n",
      "Train Epoch: 171\tLoss: 0.210766\n",
      "Train Epoch: 172\tLoss: 0.210252\n",
      "Train Epoch: 173\tLoss: 0.209534\n",
      "Train Epoch: 174\tLoss: 0.209004\n",
      "Train Epoch: 175\tLoss: 0.208280\n",
      "Train Epoch: 176\tLoss: 0.207739\n",
      "Train Epoch: 177\tLoss: 0.207010\n",
      "Train Epoch: 178\tLoss: 0.206463\n",
      "Train Epoch: 179\tLoss: 0.205727\n",
      "Train Epoch: 180\tLoss: 0.205170\n",
      "Train Epoch: 181\tLoss: 0.204437\n",
      "Train Epoch: 182\tLoss: 0.203865\n",
      "Train Epoch: 183\tLoss: 0.203138\n",
      "Train Epoch: 184\tLoss: 0.202561\n",
      "Train Epoch: 185\tLoss: 0.201841\n",
      "Train Epoch: 186\tLoss: 0.201266\n",
      "Train Epoch: 187\tLoss: 0.200550\n",
      "Train Epoch: 188\tLoss: 0.199974\n",
      "Train Epoch: 189\tLoss: 0.199279\n",
      "Train Epoch: 190\tLoss: 0.198711\n",
      "Train Epoch: 191\tLoss: 0.198031\n",
      "Train Epoch: 192\tLoss: 0.197464\n",
      "Train Epoch: 193\tLoss: 0.196799\n",
      "Train Epoch: 194\tLoss: 0.196223\n",
      "Train Epoch: 195\tLoss: 0.195573\n",
      "Train Epoch: 196\tLoss: 0.195000\n",
      "Train Epoch: 197\tLoss: 0.194363\n",
      "Train Epoch: 198\tLoss: 0.193804\n",
      "Train Epoch: 199\tLoss: 0.193178\n",
      "Train Epoch: 200\tLoss: 0.192623\n",
      "Train Epoch: 201\tLoss: 0.192030\n",
      "Train Epoch: 202\tLoss: 0.191495\n",
      "Train Epoch: 203\tLoss: 0.190922\n",
      "Train Epoch: 204\tLoss: 0.190400\n",
      "Train Epoch: 205\tLoss: 0.189842\n",
      "Train Epoch: 206\tLoss: 0.189325\n",
      "Train Epoch: 207\tLoss: 0.188783\n",
      "Train Epoch: 208\tLoss: 0.188279\n",
      "Train Epoch: 209\tLoss: 0.187757\n",
      "Train Epoch: 210\tLoss: 0.187268\n",
      "Train Epoch: 211\tLoss: 0.186764\n",
      "Train Epoch: 212\tLoss: 0.186281\n",
      "Train Epoch: 213\tLoss: 0.185789\n",
      "Train Epoch: 214\tLoss: 0.185320\n",
      "Train Epoch: 215\tLoss: 0.184841\n",
      "Train Epoch: 216\tLoss: 0.184377\n",
      "Train Epoch: 217\tLoss: 0.183909\n",
      "Train Epoch: 218\tLoss: 0.183458\n",
      "Train Epoch: 219\tLoss: 0.182996\n",
      "Train Epoch: 220\tLoss: 0.182553\n",
      "Train Epoch: 221\tLoss: 0.182102\n",
      "Train Epoch: 222\tLoss: 0.181671\n",
      "Train Epoch: 223\tLoss: 0.181229\n",
      "Train Epoch: 224\tLoss: 0.180806\n",
      "Train Epoch: 225\tLoss: 0.180371\n",
      "Train Epoch: 226\tLoss: 0.179954\n",
      "Train Epoch: 227\tLoss: 0.179529\n",
      "Train Epoch: 228\tLoss: 0.179119\n",
      "Train Epoch: 229\tLoss: 0.178704\n",
      "Train Epoch: 230\tLoss: 0.178302\n",
      "Train Epoch: 231\tLoss: 0.177895\n",
      "Train Epoch: 232\tLoss: 0.177501\n",
      "Train Epoch: 233\tLoss: 0.177102\n",
      "Train Epoch: 234\tLoss: 0.176711\n",
      "Train Epoch: 235\tLoss: 0.176315\n",
      "Train Epoch: 236\tLoss: 0.175931\n",
      "Train Epoch: 237\tLoss: 0.175544\n",
      "Train Epoch: 238\tLoss: 0.175167\n",
      "Train Epoch: 239\tLoss: 0.174785\n",
      "Train Epoch: 240\tLoss: 0.174413\n",
      "Train Epoch: 241\tLoss: 0.174035\n",
      "Train Epoch: 242\tLoss: 0.173676\n",
      "Train Epoch: 243\tLoss: 0.173302\n",
      "Train Epoch: 244\tLoss: 0.172949\n",
      "Train Epoch: 245\tLoss: 0.172577\n",
      "Train Epoch: 246\tLoss: 0.172226\n",
      "Train Epoch: 247\tLoss: 0.171862\n",
      "Train Epoch: 248\tLoss: 0.171518\n",
      "Train Epoch: 249\tLoss: 0.171158\n",
      "Train Epoch: 250\tLoss: 0.170816\n",
      "Train Epoch: 251\tLoss: 0.170458\n",
      "Train Epoch: 252\tLoss: 0.170120\n",
      "Train Epoch: 253\tLoss: 0.169769\n",
      "Train Epoch: 254\tLoss: 0.169433\n",
      "Train Epoch: 255\tLoss: 0.169081\n",
      "Train Epoch: 256\tLoss: 0.168743\n",
      "Train Epoch: 257\tLoss: 0.168392\n",
      "Train Epoch: 258\tLoss: 0.168058\n",
      "Train Epoch: 259\tLoss: 0.167711\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 260\tLoss: 0.167385\n",
      "Train Epoch: 261\tLoss: 0.167042\n",
      "Train Epoch: 262\tLoss: 0.166719\n",
      "Train Epoch: 263\tLoss: 0.166390\n",
      "Train Epoch: 264\tLoss: 0.166077\n",
      "Train Epoch: 265\tLoss: 0.165755\n",
      "Train Epoch: 266\tLoss: 0.165451\n",
      "Train Epoch: 267\tLoss: 0.165129\n",
      "Train Epoch: 268\tLoss: 0.164831\n",
      "Train Epoch: 269\tLoss: 0.164512\n",
      "Train Epoch: 270\tLoss: 0.164217\n",
      "Train Epoch: 271\tLoss: 0.163905\n",
      "Train Epoch: 272\tLoss: 0.163613\n",
      "Train Epoch: 273\tLoss: 0.163305\n",
      "Train Epoch: 274\tLoss: 0.163024\n",
      "Train Epoch: 275\tLoss: 0.162722\n",
      "Train Epoch: 276\tLoss: 0.162443\n",
      "Train Epoch: 277\tLoss: 0.162143\n",
      "Train Epoch: 278\tLoss: 0.161863\n",
      "Train Epoch: 279\tLoss: 0.161568\n",
      "Train Epoch: 280\tLoss: 0.161284\n",
      "Train Epoch: 281\tLoss: 0.160990\n",
      "Train Epoch: 282\tLoss: 0.160704\n",
      "Train Epoch: 283\tLoss: 0.160412\n",
      "Train Epoch: 284\tLoss: 0.160127\n",
      "Train Epoch: 285\tLoss: 0.159837\n",
      "Train Epoch: 286\tLoss: 0.159557\n",
      "Train Epoch: 287\tLoss: 0.159265\n",
      "Train Epoch: 288\tLoss: 0.158979\n",
      "Train Epoch: 289\tLoss: 0.158686\n",
      "Train Epoch: 290\tLoss: 0.158391\n",
      "Train Epoch: 291\tLoss: 0.158096\n",
      "Train Epoch: 292\tLoss: 0.157790\n",
      "Train Epoch: 293\tLoss: 0.157496\n",
      "Train Epoch: 294\tLoss: 0.157187\n",
      "Train Epoch: 295\tLoss: 0.156887\n",
      "Train Epoch: 296\tLoss: 0.156568\n",
      "Train Epoch: 297\tLoss: 0.156265\n",
      "Train Epoch: 298\tLoss: 0.155954\n",
      "Train Epoch: 299\tLoss: 0.155657\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(300):\n",
    "    train(model, X_train, y_train, optimizer, criterion, epoch, 'print')\n",
    "#    test(model, X_test, y_test, criterion, 'print')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.1701\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(0.1701)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test(model, X_test, y_test, criterion, 'print')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Net(300, 10)\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "min_error = 999\n",
    "epoch = count = 0\n",
    "train_loss_vec = []\n",
    "test_loss_vec = []\n",
    "\n",
    "while (epoch < 100 and count < 30):\n",
    "    train_loss = train(model, X_train, y_train, optimizer, criterion, epoch, '')\n",
    "    test_loss = test(model, X_test, y_test, criterion)\n",
    "    train_loss_vec.append(train_loss)\n",
    "    test_loss_vec.append(test_loss)\n",
    "    epoch += 1\n",
    "    if test_loss >= min_error:\n",
    "        count += 1\n",
    "    else:\n",
    "        min_error = test_loss\n",
    "        \n",
    "test(model, X_test, y_test, criterion, 'print')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(train_loss_vec, \"r\")\n",
    "plt.hold = True\n",
    "plt.plot(test_loss_vec, \"b\")\n",
    "plt.hold = False\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(1)\n",
    "for i, img in enumerate(X_test[:10]):\n",
    "    ax = fig.add_subplot(1,10,i+1)\n",
    "    ax.set_axis_off()\n",
    "    ax = plt.imshow(img)\n",
    "    with torch.no_grad():\n",
    "        a = model(img)\n",
    "    print(mnist_testset.classes[torch.argmax(a, dim=1)])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = random.randint(0,10000)\n",
    "plt.imshow(X_test[a])\n",
    "with torch.no_grad():\n",
    "    b = model(X_test[a])\n",
    "print(mnist_testset.classes[torch.argmax(b, dim=1)])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
