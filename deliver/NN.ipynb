{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.datasets as datasets\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "torch.random.seed = 42\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the MNIST dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_trainset = datasets.MNIST(root='../data', train=True, download=True,\n",
    "                                transform=None)\n",
    "\n",
    "mnist_testset = datasets.MNIST(root='../data', train=False, download=True,\n",
    "                               transform=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset MNIST\n",
      "    Number of datapoints: 60000\n",
      "    Split: train\n",
      "    Root Location: ../data\n",
      "    Transforms (if any): None\n",
      "    Target Transforms (if any): None\n",
      "\n",
      "Dataset MNIST\n",
      "    Number of datapoints: 10000\n",
      "    Split: test\n",
      "    Root Location: ../data\n",
      "    Transforms (if any): None\n",
      "    Target Transforms (if any): None\n"
     ]
    }
   ],
   "source": [
    "print(mnist_trainset)\n",
    "print('')\n",
    "print(mnist_testset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([28, 28])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist_trainset.data[0].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "size_len = mnist_trainset.data[0].size()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 5 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure(1)\n",
    "for i, img in enumerate(mnist_trainset.data[0:5]):\n",
    "    ax = fig.add_subplot(1,5,i+1)\n",
    "    ax.set_axis_off()\n",
    "    ax = plt.imshow(img)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Net architecture and train/test routines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    \"\"\"MLP with 3 ReLU hidden layers and 1 softmax output layer\"\"\"\n",
    "    \n",
    "    def __init__(self, H, C):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(size_len*size_len, H)\n",
    "        self.fc2 = nn.Linear(H, H)\n",
    "        self.fc3 = nn.Linear(H, H)\n",
    "        self.fc4 = nn.Linear(H, C)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, size_len*size_len)\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.relu(self.fc3(x))\n",
    "        x = self.softmax(self.fc4(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, x_train, y_train, optimizer, criterion, epoch, disp=''):\n",
    "    model.train()\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    output = model(x_train)\n",
    "    loss = criterion(output, y_train)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if disp=='print':\n",
    "        print(\"Train Epoch: {}\\tLoss: {:.6f}\".format(epoch, loss.item()))\n",
    "    elif disp=='graph':\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, x_test, y_test, criterion, disp=''):\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = model(x_test)\n",
    "        test_loss = criterion(output, y_test)\n",
    "\n",
    "    if disp=='print':\n",
    "        print(\"\\nTest set: Average loss: {:.4f}\\n\".format(test_loss))\n",
    "    elif disp=='graph':\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = mnist_trainset.data.float()\n",
    "y_train = mnist_trainset.targets\n",
    "\n",
    "X_test = mnist_testset.data.float()\n",
    "y_test = mnist_testset.targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (fc1): Linear(in_features=784, out_features=100, bias=True)\n",
      "  (fc2): Linear(in_features=100, out_features=100, bias=True)\n",
      "  (fc3): Linear(in_features=100, out_features=100, bias=True)\n",
      "  (fc4): Linear(in_features=100, out_features=10, bias=True)\n",
      "  (relu): ReLU()\n",
      "  (softmax): LogSoftmax()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = Net(100, 10)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.01\n",
    "gamma = 10\n",
    "max_epoch = 100\n",
    "optimizer = optim.SGD(model.parameters(), lr=alpha)\n",
    "criterion = torch.nn.NLLLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch = count = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0\tLoss: 5.474500\n",
      "\n",
      "Test set: Average loss: 5.6826\n",
      "\n",
      "Train Epoch: 1\tLoss: 5.634797\n",
      "\n",
      "Test set: Average loss: 8.1347\n",
      "\n",
      "Train Epoch: 2\tLoss: 7.918168\n",
      "\n",
      "Test set: Average loss: 4.9506\n",
      "\n",
      "Train Epoch: 3\tLoss: 4.898543\n",
      "\n",
      "Test set: Average loss: 3.4923\n",
      "\n",
      "Train Epoch: 4\tLoss: 3.429244\n",
      "\n",
      "Test set: Average loss: 2.3844\n",
      "\n",
      "Train Epoch: 5\tLoss: 2.375289\n",
      "\n",
      "Test set: Average loss: 1.9288\n",
      "\n",
      "Train Epoch: 6\tLoss: 1.926137\n",
      "\n",
      "Test set: Average loss: 1.6878\n",
      "\n",
      "Train Epoch: 7\tLoss: 1.688768\n",
      "\n",
      "Test set: Average loss: 1.5129\n",
      "\n",
      "Train Epoch: 8\tLoss: 1.519757\n",
      "\n",
      "Test set: Average loss: 1.3514\n",
      "\n",
      "Train Epoch: 9\tLoss: 1.360964\n",
      "\n",
      "Test set: Average loss: 1.2053\n",
      "\n",
      "Train Epoch: 10\tLoss: 1.219004\n",
      "\n",
      "Test set: Average loss: 1.0813\n",
      "\n",
      "Train Epoch: 11\tLoss: 1.098781\n",
      "\n",
      "Test set: Average loss: 0.9798\n",
      "\n",
      "Train Epoch: 12\tLoss: 0.998549\n",
      "\n",
      "Test set: Average loss: 0.8963\n",
      "\n",
      "Train Epoch: 13\tLoss: 0.918295\n",
      "\n",
      "Test set: Average loss: 0.8378\n",
      "\n",
      "Train Epoch: 14\tLoss: 0.856852\n",
      "\n",
      "Test set: Average loss: 0.7988\n",
      "\n",
      "Train Epoch: 15\tLoss: 0.824666\n",
      "\n",
      "Test set: Average loss: 0.8532\n",
      "\n",
      "Train Epoch: 16\tLoss: 0.864336\n",
      "\n",
      "Test set: Average loss: 1.1067\n",
      "\n",
      "Train Epoch: 17\tLoss: 1.132358\n",
      "\n",
      "Test set: Average loss: 1.1341\n",
      "\n",
      "Train Epoch: 18\tLoss: 1.129959\n",
      "\n",
      "Test set: Average loss: 0.9198\n",
      "\n",
      "Train Epoch: 19\tLoss: 0.929410\n",
      "\n",
      "Test set: Average loss: 0.7578\n",
      "\n",
      "Train Epoch: 20\tLoss: 0.780886\n",
      "\n",
      "Test set: Average loss: 0.7436\n",
      "\n",
      "Train Epoch: 21\tLoss: 0.755798\n",
      "\n",
      "Test set: Average loss: 0.7086\n",
      "\n",
      "Train Epoch: 22\tLoss: 0.729336\n",
      "\n",
      "Test set: Average loss: 0.6980\n",
      "\n",
      "Train Epoch: 23\tLoss: 0.711790\n",
      "\n",
      "Test set: Average loss: 0.6656\n",
      "\n",
      "Train Epoch: 24\tLoss: 0.681488\n",
      "\n",
      "Test set: Average loss: 0.6303\n",
      "\n",
      "Train Epoch: 25\tLoss: 0.647613\n",
      "\n",
      "Test set: Average loss: 0.5949\n",
      "\n",
      "Train Epoch: 26\tLoss: 0.610650\n",
      "\n",
      "Test set: Average loss: 0.5702\n",
      "\n",
      "Train Epoch: 27\tLoss: 0.589467\n",
      "\n",
      "Test set: Average loss: 0.5566\n",
      "\n",
      "Train Epoch: 28\tLoss: 0.572639\n",
      "\n",
      "Test set: Average loss: 0.5458\n",
      "\n",
      "Train Epoch: 29\tLoss: 0.565547\n",
      "\n",
      "Test set: Average loss: 0.5501\n",
      "\n",
      "Train Epoch: 30\tLoss: 0.564995\n",
      "\n",
      "Test set: Average loss: 0.5438\n",
      "\n",
      "Train Epoch: 31\tLoss: 0.563728\n",
      "\n",
      "Test set: Average loss: 0.5637\n",
      "\n",
      "Train Epoch: 32\tLoss: 0.576209\n",
      "\n",
      "Test set: Average loss: 0.5503\n",
      "\n",
      "Train Epoch: 33\tLoss: 0.570060\n",
      "\n",
      "Test set: Average loss: 0.5634\n",
      "\n",
      "Train Epoch: 34\tLoss: 0.574046\n",
      "\n",
      "Test set: Average loss: 0.5264\n",
      "\n",
      "Train Epoch: 35\tLoss: 0.546340\n",
      "\n",
      "Test set: Average loss: 0.5153\n",
      "\n",
      "Train Epoch: 36\tLoss: 0.526771\n",
      "\n",
      "Test set: Average loss: 0.4783\n",
      "\n",
      "Train Epoch: 37\tLoss: 0.498420\n",
      "\n",
      "Test set: Average loss: 0.4664\n",
      "\n",
      "Train Epoch: 38\tLoss: 0.478924\n",
      "\n",
      "Test set: Average loss: 0.4430\n",
      "\n",
      "Train Epoch: 39\tLoss: 0.462588\n",
      "\n",
      "Test set: Average loss: 0.4376\n",
      "\n",
      "Train Epoch: 40\tLoss: 0.450493\n",
      "\n",
      "Test set: Average loss: 0.4222\n",
      "\n",
      "Train Epoch: 41\tLoss: 0.440821\n",
      "\n",
      "Test set: Average loss: 0.4202\n",
      "\n",
      "Train Epoch: 42\tLoss: 0.433067\n",
      "\n",
      "Test set: Average loss: 0.4085\n",
      "\n",
      "Train Epoch: 43\tLoss: 0.426319\n",
      "\n",
      "Test set: Average loss: 0.4084\n",
      "\n",
      "Train Epoch: 44\tLoss: 0.420882\n",
      "\n",
      "Test set: Average loss: 0.3985\n",
      "\n",
      "Train Epoch: 45\tLoss: 0.415817\n",
      "\n",
      "Test set: Average loss: 0.4001\n",
      "\n",
      "Train Epoch: 46\tLoss: 0.411810\n",
      "\n",
      "Test set: Average loss: 0.3909\n",
      "\n",
      "Train Epoch: 47\tLoss: 0.407892\n",
      "\n",
      "Test set: Average loss: 0.3945\n",
      "\n",
      "Train Epoch: 48\tLoss: 0.405331\n",
      "\n",
      "Test set: Average loss: 0.3856\n",
      "\n",
      "Train Epoch: 49\tLoss: 0.402443\n",
      "\n",
      "Test set: Average loss: 0.3926\n",
      "\n",
      "Train Epoch: 50\tLoss: 0.402278\n",
      "\n",
      "Test set: Average loss: 0.3837\n",
      "\n",
      "Train Epoch: 51\tLoss: 0.400807\n",
      "\n",
      "Test set: Average loss: 0.3965\n",
      "\n",
      "Train Epoch: 52\tLoss: 0.404684\n",
      "\n",
      "Test set: Average loss: 0.3866\n",
      "\n",
      "Train Epoch: 53\tLoss: 0.404166\n",
      "\n",
      "Test set: Average loss: 0.4079\n",
      "\n",
      "Train Epoch: 54\tLoss: 0.414379\n",
      "\n",
      "Test set: Average loss: 0.3940\n",
      "\n",
      "Train Epoch: 55\tLoss: 0.411973\n",
      "\n",
      "Test set: Average loss: 0.4240\n",
      "\n",
      "Train Epoch: 56\tLoss: 0.428843\n",
      "\n",
      "Test set: Average loss: 0.4002\n",
      "\n",
      "Train Epoch: 57\tLoss: 0.418013\n",
      "\n",
      "Test set: Average loss: 0.4288\n",
      "\n",
      "Train Epoch: 58\tLoss: 0.432972\n",
      "\n",
      "Test set: Average loss: 0.3923\n",
      "\n",
      "Train Epoch: 59\tLoss: 0.408776\n",
      "\n",
      "Test set: Average loss: 0.4052\n",
      "\n",
      "Train Epoch: 60\tLoss: 0.410473\n",
      "\n",
      "Test set: Average loss: 0.3724\n",
      "\n",
      "Train Epoch: 61\tLoss: 0.386512\n",
      "\n",
      "Test set: Average loss: 0.3738\n",
      "\n",
      "Train Epoch: 62\tLoss: 0.380734\n",
      "\n",
      "Test set: Average loss: 0.3547\n",
      "\n",
      "Train Epoch: 63\tLoss: 0.366901\n",
      "\n",
      "Test set: Average loss: 0.3534\n",
      "\n",
      "Train Epoch: 64\tLoss: 0.361382\n",
      "\n",
      "Test set: Average loss: 0.3433\n",
      "\n",
      "Train Epoch: 65\tLoss: 0.354251\n",
      "\n",
      "Test set: Average loss: 0.3416\n",
      "\n",
      "Train Epoch: 66\tLoss: 0.350003\n",
      "\n",
      "Test set: Average loss: 0.3355\n",
      "\n",
      "Train Epoch: 67\tLoss: 0.345541\n",
      "\n",
      "Test set: Average loss: 0.3337\n",
      "\n",
      "Train Epoch: 68\tLoss: 0.342181\n",
      "\n",
      "Test set: Average loss: 0.3296\n",
      "\n",
      "Train Epoch: 69\tLoss: 0.338912\n",
      "\n",
      "Test set: Average loss: 0.3277\n",
      "\n",
      "Train Epoch: 70\tLoss: 0.336103\n",
      "\n",
      "Test set: Average loss: 0.3246\n",
      "\n",
      "Train Epoch: 71\tLoss: 0.333415\n",
      "\n",
      "Test set: Average loss: 0.3227\n",
      "\n",
      "Train Epoch: 72\tLoss: 0.330954\n",
      "\n",
      "Test set: Average loss: 0.3203\n",
      "\n",
      "Train Epoch: 73\tLoss: 0.328576\n",
      "\n",
      "Test set: Average loss: 0.3183\n",
      "\n",
      "Train Epoch: 74\tLoss: 0.326308\n",
      "\n",
      "Test set: Average loss: 0.3162\n",
      "\n",
      "Train Epoch: 75\tLoss: 0.324156\n",
      "\n",
      "Test set: Average loss: 0.3143\n",
      "\n",
      "Train Epoch: 76\tLoss: 0.322026\n",
      "\n",
      "Test set: Average loss: 0.3125\n",
      "\n",
      "Train Epoch: 77\tLoss: 0.319997\n",
      "\n",
      "Test set: Average loss: 0.3105\n",
      "\n",
      "Train Epoch: 78\tLoss: 0.317993\n",
      "\n",
      "Test set: Average loss: 0.3089\n",
      "\n",
      "Train Epoch: 79\tLoss: 0.316087\n",
      "\n",
      "Test set: Average loss: 0.3070\n",
      "\n",
      "Train Epoch: 80\tLoss: 0.314198\n",
      "\n",
      "Test set: Average loss: 0.3056\n",
      "\n",
      "Train Epoch: 81\tLoss: 0.312419\n",
      "\n",
      "Test set: Average loss: 0.3038\n",
      "\n",
      "Train Epoch: 82\tLoss: 0.310656\n",
      "\n",
      "Test set: Average loss: 0.3025\n",
      "\n",
      "Train Epoch: 83\tLoss: 0.308983\n",
      "\n",
      "Test set: Average loss: 0.3008\n",
      "\n",
      "Train Epoch: 84\tLoss: 0.307318\n",
      "\n",
      "Test set: Average loss: 0.2996\n",
      "\n",
      "Train Epoch: 85\tLoss: 0.305767\n",
      "\n",
      "Test set: Average loss: 0.2981\n",
      "\n",
      "Train Epoch: 86\tLoss: 0.304211\n",
      "\n",
      "Test set: Average loss: 0.2970\n",
      "\n",
      "Train Epoch: 87\tLoss: 0.302791\n",
      "\n",
      "Test set: Average loss: 0.2956\n",
      "\n",
      "Train Epoch: 88\tLoss: 0.301331\n",
      "\n",
      "Test set: Average loss: 0.2945\n",
      "\n",
      "Train Epoch: 89\tLoss: 0.300002\n",
      "\n",
      "Test set: Average loss: 0.2933\n",
      "\n",
      "Train Epoch: 90\tLoss: 0.298628\n",
      "\n",
      "Test set: Average loss: 0.2922\n",
      "\n",
      "Train Epoch: 91\tLoss: 0.297418\n",
      "\n",
      "Test set: Average loss: 0.2912\n",
      "\n",
      "Train Epoch: 92\tLoss: 0.296116\n",
      "\n",
      "Test set: Average loss: 0.2901\n",
      "\n",
      "Train Epoch: 93\tLoss: 0.295025\n",
      "\n",
      "Test set: Average loss: 0.2893\n",
      "\n",
      "Train Epoch: 94\tLoss: 0.293825\n",
      "\n",
      "Test set: Average loss: 0.2882\n",
      "\n",
      "Train Epoch: 95\tLoss: 0.292869\n",
      "\n",
      "Test set: Average loss: 0.2877\n",
      "\n",
      "Train Epoch: 96\tLoss: 0.291764\n",
      "\n",
      "Test set: Average loss: 0.2866\n",
      "\n",
      "Train Epoch: 97\tLoss: 0.290928\n",
      "\n",
      "Test set: Average loss: 0.2863\n",
      "\n",
      "Train Epoch: 98\tLoss: 0.289967\n",
      "\n",
      "Test set: Average loss: 0.2852\n",
      "\n",
      "Train Epoch: 99\tLoss: 0.289274\n",
      "\n",
      "Test set: Average loss: 0.2852\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(100):\n",
    "    train(model, X_train, y_train, optimizer, criterion, epoch)\n",
    "    test(model, X_test, y_test, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
