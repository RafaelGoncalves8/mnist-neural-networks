{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.datasets as datasets\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "torch.random.seed = 42\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the MNIST dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_trainset = datasets.MNIST(root='../data', train=True, download=True,\n",
    "                                transform=None)\n",
    "\n",
    "mnist_testset = datasets.MNIST(root='../data', train=False, download=True,\n",
    "                               transform=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset MNIST\n",
      "    Number of datapoints: 60000\n",
      "    Split: train\n",
      "    Root Location: ../data\n",
      "    Transforms (if any): None\n",
      "    Target Transforms (if any): None\n",
      "\n",
      "Dataset MNIST\n",
      "    Number of datapoints: 10000\n",
      "    Split: test\n",
      "    Root Location: ../data\n",
      "    Transforms (if any): None\n",
      "    Target Transforms (if any): None\n"
     ]
    }
   ],
   "source": [
    "print(mnist_trainset)\n",
    "print('')\n",
    "print(mnist_testset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([28, 28])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist_trainset.data[0].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "size_len = mnist_trainset.data[0].size()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 5 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure(1)\n",
    "for i, img in enumerate(mnist_trainset.data[0:5]):\n",
    "    ax = fig.add_subplot(1,5,i+1)\n",
    "    ax.set_axis_off()\n",
    "    ax = plt.imshow(img)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Net architecture and train/test routines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    \"\"\"MLP with 3 ReLU hidden layers and 1 softmax output layer\"\"\"\n",
    "    \n",
    "    def __init__(self, H, C):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(size_len*size_len, H)\n",
    "        self.fc2 = nn.Linear(H, H)\n",
    "        self.fc3 = nn.Linear(H, H)\n",
    "        self.fc4 = nn.Linear(H, C)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, size_len*size_len)\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.relu(self.fc3(x))\n",
    "        x = self.softmax(self.fc4(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, x_train, y_train, optimizer, criterion, epoch, disp=''):\n",
    "    model.train()\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    output = model(x_train)\n",
    "    loss = criterion(output, y_train)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if disp=='print':\n",
    "        print(\"Train Epoch: {}\\tLoss: {:.6f}\".format(epoch, loss.item()))\n",
    "    elif disp=='graph':\n",
    "        pass\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, x_test, y_test, criterion, disp=''):\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = model(x_test)\n",
    "        test_loss = criterion(output, y_test)\n",
    "\n",
    "    if disp=='print':\n",
    "        print(\"\\nTest set: Average loss: {:.4f}\\n\".format(test_loss))\n",
    "    elif disp=='graph':\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = mnist_trainset.data.float()\n",
    "y_train = mnist_trainset.targets\n",
    "\n",
    "X_test = mnist_testset.data.float()\n",
    "y_test = mnist_testset.targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (fc1): Linear(in_features=784, out_features=100, bias=True)\n",
      "  (fc2): Linear(in_features=100, out_features=100, bias=True)\n",
      "  (fc3): Linear(in_features=100, out_features=100, bias=True)\n",
      "  (fc4): Linear(in_features=100, out_features=10, bias=True)\n",
      "  (relu): ReLU()\n",
      "  (softmax): LogSoftmax()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = Net(100, 10)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.006\n",
    "gamma = 10\n",
    "max_epoch = 100\n",
    "optimizer = optim.SGD(model.parameters(), lr=alpha)\n",
    "criterion = torch.nn.NLLLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch = count = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0\tLoss: 4.391118\n",
      "Train Epoch: 1\tLoss: 4.260060\n",
      "Train Epoch: 2\tLoss: 3.790262\n",
      "Train Epoch: 3\tLoss: 2.499252\n",
      "Train Epoch: 4\tLoss: 1.925754\n",
      "Train Epoch: 5\tLoss: 1.703450\n",
      "Train Epoch: 6\tLoss: 1.556812\n",
      "Train Epoch: 7\tLoss: 1.427120\n",
      "Train Epoch: 8\tLoss: 1.307750\n",
      "Train Epoch: 9\tLoss: 1.198856\n",
      "Train Epoch: 10\tLoss: 1.102863\n",
      "Train Epoch: 11\tLoss: 1.021212\n",
      "Train Epoch: 12\tLoss: 0.952294\n",
      "Train Epoch: 13\tLoss: 0.894315\n",
      "Train Epoch: 14\tLoss: 0.845208\n",
      "Train Epoch: 15\tLoss: 0.803080\n",
      "Train Epoch: 16\tLoss: 0.766598\n",
      "Train Epoch: 17\tLoss: 0.734833\n",
      "Train Epoch: 18\tLoss: 0.706904\n",
      "Train Epoch: 19\tLoss: 0.682151\n",
      "Train Epoch: 20\tLoss: 0.660052\n",
      "Train Epoch: 21\tLoss: 0.640175\n",
      "Train Epoch: 22\tLoss: 0.622220\n",
      "Train Epoch: 23\tLoss: 0.605932\n",
      "Train Epoch: 24\tLoss: 0.591040\n",
      "Train Epoch: 25\tLoss: 0.577365\n",
      "Train Epoch: 26\tLoss: 0.564783\n",
      "Train Epoch: 27\tLoss: 0.553130\n",
      "Train Epoch: 28\tLoss: 0.542316\n",
      "Train Epoch: 29\tLoss: 0.532247\n",
      "Train Epoch: 30\tLoss: 0.522840\n",
      "Train Epoch: 31\tLoss: 0.514046\n",
      "Train Epoch: 32\tLoss: 0.505836\n",
      "Train Epoch: 33\tLoss: 0.498185\n",
      "Train Epoch: 34\tLoss: 0.491141\n",
      "Train Epoch: 35\tLoss: 0.484837\n",
      "Train Epoch: 36\tLoss: 0.479458\n",
      "Train Epoch: 37\tLoss: 0.475472\n",
      "Train Epoch: 38\tLoss: 0.473547\n",
      "Train Epoch: 39\tLoss: 0.475161\n",
      "Train Epoch: 40\tLoss: 0.482559\n",
      "Train Epoch: 41\tLoss: 0.499806\n",
      "Train Epoch: 42\tLoss: 0.530764\n",
      "Train Epoch: 43\tLoss: 0.578297\n",
      "Train Epoch: 44\tLoss: 0.628491\n",
      "Train Epoch: 45\tLoss: 0.631331\n",
      "Train Epoch: 46\tLoss: 0.574852\n",
      "Train Epoch: 47\tLoss: 0.495555\n",
      "Train Epoch: 48\tLoss: 0.447440\n",
      "Train Epoch: 49\tLoss: 0.425771\n",
      "Train Epoch: 50\tLoss: 0.415384\n",
      "Train Epoch: 51\tLoss: 0.409023\n",
      "Train Epoch: 52\tLoss: 0.404163\n",
      "Train Epoch: 53\tLoss: 0.399997\n",
      "Train Epoch: 54\tLoss: 0.396186\n",
      "Train Epoch: 55\tLoss: 0.392607\n",
      "Train Epoch: 56\tLoss: 0.389198\n",
      "Train Epoch: 57\tLoss: 0.385931\n",
      "Train Epoch: 58\tLoss: 0.382774\n",
      "Train Epoch: 59\tLoss: 0.379722\n",
      "Train Epoch: 60\tLoss: 0.376772\n",
      "Train Epoch: 61\tLoss: 0.373907\n",
      "Train Epoch: 62\tLoss: 0.371124\n",
      "Train Epoch: 63\tLoss: 0.368413\n",
      "Train Epoch: 64\tLoss: 0.365780\n",
      "Train Epoch: 65\tLoss: 0.363212\n",
      "Train Epoch: 66\tLoss: 0.360710\n",
      "Train Epoch: 67\tLoss: 0.358277\n",
      "Train Epoch: 68\tLoss: 0.355911\n",
      "Train Epoch: 69\tLoss: 0.353601\n",
      "Train Epoch: 70\tLoss: 0.351340\n",
      "Train Epoch: 71\tLoss: 0.349132\n",
      "Train Epoch: 72\tLoss: 0.346968\n",
      "Train Epoch: 73\tLoss: 0.344851\n",
      "Train Epoch: 74\tLoss: 0.342781\n",
      "Train Epoch: 75\tLoss: 0.340754\n",
      "Train Epoch: 76\tLoss: 0.338767\n",
      "Train Epoch: 77\tLoss: 0.336817\n",
      "Train Epoch: 78\tLoss: 0.334906\n",
      "Train Epoch: 79\tLoss: 0.333032\n",
      "Train Epoch: 80\tLoss: 0.331187\n",
      "Train Epoch: 81\tLoss: 0.329380\n",
      "Train Epoch: 82\tLoss: 0.327603\n",
      "Train Epoch: 83\tLoss: 0.325857\n",
      "Train Epoch: 84\tLoss: 0.324141\n",
      "Train Epoch: 85\tLoss: 0.322452\n",
      "Train Epoch: 86\tLoss: 0.320795\n",
      "Train Epoch: 87\tLoss: 0.319167\n",
      "Train Epoch: 88\tLoss: 0.317569\n",
      "Train Epoch: 89\tLoss: 0.315992\n",
      "Train Epoch: 90\tLoss: 0.314440\n",
      "Train Epoch: 91\tLoss: 0.312914\n",
      "Train Epoch: 92\tLoss: 0.311413\n",
      "Train Epoch: 93\tLoss: 0.309938\n",
      "Train Epoch: 94\tLoss: 0.308487\n",
      "Train Epoch: 95\tLoss: 0.307057\n",
      "Train Epoch: 96\tLoss: 0.305653\n",
      "Train Epoch: 97\tLoss: 0.304268\n",
      "Train Epoch: 98\tLoss: 0.302904\n",
      "Train Epoch: 99\tLoss: 0.301562\n",
      "Train Epoch: 100\tLoss: 0.300238\n",
      "Train Epoch: 101\tLoss: 0.298936\n",
      "Train Epoch: 102\tLoss: 0.297654\n",
      "Train Epoch: 103\tLoss: 0.296391\n",
      "Train Epoch: 104\tLoss: 0.295143\n",
      "Train Epoch: 105\tLoss: 0.293909\n",
      "Train Epoch: 106\tLoss: 0.292696\n",
      "Train Epoch: 107\tLoss: 0.291498\n",
      "Train Epoch: 108\tLoss: 0.290315\n",
      "Train Epoch: 109\tLoss: 0.289148\n",
      "Train Epoch: 110\tLoss: 0.287994\n",
      "Train Epoch: 111\tLoss: 0.286855\n",
      "Train Epoch: 112\tLoss: 0.285730\n",
      "Train Epoch: 113\tLoss: 0.284614\n",
      "Train Epoch: 114\tLoss: 0.283510\n",
      "Train Epoch: 115\tLoss: 0.282421\n",
      "Train Epoch: 116\tLoss: 0.281343\n",
      "Train Epoch: 117\tLoss: 0.280282\n",
      "Train Epoch: 118\tLoss: 0.279227\n",
      "Train Epoch: 119\tLoss: 0.278188\n",
      "Train Epoch: 120\tLoss: 0.277160\n",
      "Train Epoch: 121\tLoss: 0.276143\n",
      "Train Epoch: 122\tLoss: 0.275137\n",
      "Train Epoch: 123\tLoss: 0.274143\n",
      "Train Epoch: 124\tLoss: 0.273160\n",
      "Train Epoch: 125\tLoss: 0.272188\n",
      "Train Epoch: 126\tLoss: 0.271227\n",
      "Train Epoch: 127\tLoss: 0.270278\n",
      "Train Epoch: 128\tLoss: 0.269336\n",
      "Train Epoch: 129\tLoss: 0.268405\n",
      "Train Epoch: 130\tLoss: 0.267480\n",
      "Train Epoch: 131\tLoss: 0.266565\n",
      "Train Epoch: 132\tLoss: 0.265660\n",
      "Train Epoch: 133\tLoss: 0.264764\n",
      "Train Epoch: 134\tLoss: 0.263879\n",
      "Train Epoch: 135\tLoss: 0.263003\n",
      "Train Epoch: 136\tLoss: 0.262136\n",
      "Train Epoch: 137\tLoss: 0.261277\n",
      "Train Epoch: 138\tLoss: 0.260426\n",
      "Train Epoch: 139\tLoss: 0.259583\n",
      "Train Epoch: 140\tLoss: 0.258749\n",
      "Train Epoch: 141\tLoss: 0.257924\n",
      "Train Epoch: 142\tLoss: 0.257107\n",
      "Train Epoch: 143\tLoss: 0.256298\n",
      "Train Epoch: 144\tLoss: 0.255496\n",
      "Train Epoch: 145\tLoss: 0.254703\n",
      "Train Epoch: 146\tLoss: 0.253917\n",
      "Train Epoch: 147\tLoss: 0.253138\n",
      "Train Epoch: 148\tLoss: 0.252367\n",
      "Train Epoch: 149\tLoss: 0.251599\n",
      "Train Epoch: 150\tLoss: 0.250840\n",
      "Train Epoch: 151\tLoss: 0.250087\n",
      "Train Epoch: 152\tLoss: 0.249343\n",
      "Train Epoch: 153\tLoss: 0.248605\n",
      "Train Epoch: 154\tLoss: 0.247875\n",
      "Train Epoch: 155\tLoss: 0.247148\n",
      "Train Epoch: 156\tLoss: 0.246431\n",
      "Train Epoch: 157\tLoss: 0.245720\n",
      "Train Epoch: 158\tLoss: 0.245013\n",
      "Train Epoch: 159\tLoss: 0.244311\n",
      "Train Epoch: 160\tLoss: 0.243615\n",
      "Train Epoch: 161\tLoss: 0.242925\n",
      "Train Epoch: 162\tLoss: 0.242241\n",
      "Train Epoch: 163\tLoss: 0.241564\n",
      "Train Epoch: 164\tLoss: 0.240892\n",
      "Train Epoch: 165\tLoss: 0.240226\n",
      "Train Epoch: 166\tLoss: 0.239566\n",
      "Train Epoch: 167\tLoss: 0.238911\n",
      "Train Epoch: 168\tLoss: 0.238263\n",
      "Train Epoch: 169\tLoss: 0.237618\n",
      "Train Epoch: 170\tLoss: 0.236978\n",
      "Train Epoch: 171\tLoss: 0.236342\n",
      "Train Epoch: 172\tLoss: 0.235710\n",
      "Train Epoch: 173\tLoss: 0.235082\n",
      "Train Epoch: 174\tLoss: 0.234459\n",
      "Train Epoch: 175\tLoss: 0.233838\n",
      "Train Epoch: 176\tLoss: 0.233223\n",
      "Train Epoch: 177\tLoss: 0.232612\n",
      "Train Epoch: 178\tLoss: 0.232009\n",
      "Train Epoch: 179\tLoss: 0.231407\n",
      "Train Epoch: 180\tLoss: 0.230809\n",
      "Train Epoch: 181\tLoss: 0.230217\n",
      "Train Epoch: 182\tLoss: 0.229627\n",
      "Train Epoch: 183\tLoss: 0.229043\n",
      "Train Epoch: 184\tLoss: 0.228463\n",
      "Train Epoch: 185\tLoss: 0.227887\n",
      "Train Epoch: 186\tLoss: 0.227317\n",
      "Train Epoch: 187\tLoss: 0.226750\n",
      "Train Epoch: 188\tLoss: 0.226188\n",
      "Train Epoch: 189\tLoss: 0.225630\n",
      "Train Epoch: 190\tLoss: 0.225075\n",
      "Train Epoch: 191\tLoss: 0.224524\n",
      "Train Epoch: 192\tLoss: 0.223976\n",
      "Train Epoch: 193\tLoss: 0.223433\n",
      "Train Epoch: 194\tLoss: 0.222894\n",
      "Train Epoch: 195\tLoss: 0.222358\n",
      "Train Epoch: 196\tLoss: 0.221825\n",
      "Train Epoch: 197\tLoss: 0.221295\n",
      "Train Epoch: 198\tLoss: 0.220770\n",
      "Train Epoch: 199\tLoss: 0.220250\n",
      "Train Epoch: 200\tLoss: 0.219731\n",
      "Train Epoch: 201\tLoss: 0.219214\n",
      "Train Epoch: 202\tLoss: 0.218704\n",
      "Train Epoch: 203\tLoss: 0.218196\n",
      "Train Epoch: 204\tLoss: 0.217692\n",
      "Train Epoch: 205\tLoss: 0.217192\n",
      "Train Epoch: 206\tLoss: 0.216695\n",
      "Train Epoch: 207\tLoss: 0.216202\n",
      "Train Epoch: 208\tLoss: 0.215713\n",
      "Train Epoch: 209\tLoss: 0.215225\n",
      "Train Epoch: 210\tLoss: 0.214740\n",
      "Train Epoch: 211\tLoss: 0.214261\n",
      "Train Epoch: 212\tLoss: 0.213784\n",
      "Train Epoch: 213\tLoss: 0.213311\n",
      "Train Epoch: 214\tLoss: 0.212839\n",
      "Train Epoch: 215\tLoss: 0.212372\n",
      "Train Epoch: 216\tLoss: 0.211908\n",
      "Train Epoch: 217\tLoss: 0.211444\n",
      "Train Epoch: 218\tLoss: 0.210983\n",
      "Train Epoch: 219\tLoss: 0.210526\n",
      "Train Epoch: 220\tLoss: 0.210072\n",
      "Train Epoch: 221\tLoss: 0.209621\n",
      "Train Epoch: 222\tLoss: 0.209171\n",
      "Train Epoch: 223\tLoss: 0.208726\n",
      "Train Epoch: 224\tLoss: 0.208281\n",
      "Train Epoch: 225\tLoss: 0.207840\n",
      "Train Epoch: 226\tLoss: 0.207401\n",
      "Train Epoch: 227\tLoss: 0.206964\n",
      "Train Epoch: 228\tLoss: 0.206528\n",
      "Train Epoch: 229\tLoss: 0.206096\n",
      "Train Epoch: 230\tLoss: 0.205666\n",
      "Train Epoch: 231\tLoss: 0.205241\n",
      "Train Epoch: 232\tLoss: 0.204817\n",
      "Train Epoch: 233\tLoss: 0.204398\n",
      "Train Epoch: 234\tLoss: 0.203981\n",
      "Train Epoch: 235\tLoss: 0.203568\n",
      "Train Epoch: 236\tLoss: 0.203155\n",
      "Train Epoch: 237\tLoss: 0.202745\n",
      "Train Epoch: 238\tLoss: 0.202339\n",
      "Train Epoch: 239\tLoss: 0.201934\n",
      "Train Epoch: 240\tLoss: 0.201531\n",
      "Train Epoch: 241\tLoss: 0.201132\n",
      "Train Epoch: 242\tLoss: 0.200736\n",
      "Train Epoch: 243\tLoss: 0.200340\n",
      "Train Epoch: 244\tLoss: 0.199948\n",
      "Train Epoch: 245\tLoss: 0.199556\n",
      "Train Epoch: 246\tLoss: 0.199169\n",
      "Train Epoch: 247\tLoss: 0.198783\n",
      "Train Epoch: 248\tLoss: 0.198400\n",
      "Train Epoch: 249\tLoss: 0.198019\n",
      "Train Epoch: 250\tLoss: 0.197639\n",
      "Train Epoch: 251\tLoss: 0.197262\n",
      "Train Epoch: 252\tLoss: 0.196886\n",
      "Train Epoch: 253\tLoss: 0.196513\n",
      "Train Epoch: 254\tLoss: 0.196142\n",
      "Train Epoch: 255\tLoss: 0.195772\n",
      "Train Epoch: 256\tLoss: 0.195405\n",
      "Train Epoch: 257\tLoss: 0.195039\n",
      "Train Epoch: 258\tLoss: 0.194674\n",
      "Train Epoch: 259\tLoss: 0.194312\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 260\tLoss: 0.193951\n",
      "Train Epoch: 261\tLoss: 0.193592\n",
      "Train Epoch: 262\tLoss: 0.193236\n",
      "Train Epoch: 263\tLoss: 0.192880\n",
      "Train Epoch: 264\tLoss: 0.192527\n",
      "Train Epoch: 265\tLoss: 0.192176\n",
      "Train Epoch: 266\tLoss: 0.191828\n",
      "Train Epoch: 267\tLoss: 0.191480\n",
      "Train Epoch: 268\tLoss: 0.191135\n",
      "Train Epoch: 269\tLoss: 0.190792\n",
      "Train Epoch: 270\tLoss: 0.190452\n",
      "Train Epoch: 271\tLoss: 0.190111\n",
      "Train Epoch: 272\tLoss: 0.189773\n",
      "Train Epoch: 273\tLoss: 0.189436\n",
      "Train Epoch: 274\tLoss: 0.189101\n",
      "Train Epoch: 275\tLoss: 0.188770\n",
      "Train Epoch: 276\tLoss: 0.188438\n",
      "Train Epoch: 277\tLoss: 0.188108\n",
      "Train Epoch: 278\tLoss: 0.187780\n",
      "Train Epoch: 279\tLoss: 0.187453\n",
      "Train Epoch: 280\tLoss: 0.187128\n",
      "Train Epoch: 281\tLoss: 0.186805\n",
      "Train Epoch: 282\tLoss: 0.186482\n",
      "Train Epoch: 283\tLoss: 0.186162\n",
      "Train Epoch: 284\tLoss: 0.185844\n",
      "Train Epoch: 285\tLoss: 0.185526\n",
      "Train Epoch: 286\tLoss: 0.185209\n",
      "Train Epoch: 287\tLoss: 0.184896\n",
      "Train Epoch: 288\tLoss: 0.184584\n",
      "Train Epoch: 289\tLoss: 0.184273\n",
      "Train Epoch: 290\tLoss: 0.183963\n",
      "Train Epoch: 291\tLoss: 0.183655\n",
      "Train Epoch: 292\tLoss: 0.183348\n",
      "Train Epoch: 293\tLoss: 0.183042\n",
      "Train Epoch: 294\tLoss: 0.182737\n",
      "Train Epoch: 295\tLoss: 0.182435\n",
      "Train Epoch: 296\tLoss: 0.182133\n",
      "Train Epoch: 297\tLoss: 0.181832\n",
      "Train Epoch: 298\tLoss: 0.181531\n",
      "Train Epoch: 299\tLoss: 0.181232\n",
      "Train Epoch: 300\tLoss: 0.180935\n",
      "Train Epoch: 301\tLoss: 0.180638\n",
      "Train Epoch: 302\tLoss: 0.180344\n",
      "Train Epoch: 303\tLoss: 0.180051\n",
      "Train Epoch: 304\tLoss: 0.179758\n",
      "Train Epoch: 305\tLoss: 0.179466\n",
      "Train Epoch: 306\tLoss: 0.179177\n",
      "Train Epoch: 307\tLoss: 0.178887\n",
      "Train Epoch: 308\tLoss: 0.178599\n",
      "Train Epoch: 309\tLoss: 0.178313\n",
      "Train Epoch: 310\tLoss: 0.178028\n",
      "Train Epoch: 311\tLoss: 0.177745\n",
      "Train Epoch: 312\tLoss: 0.177461\n",
      "Train Epoch: 313\tLoss: 0.177181\n",
      "Train Epoch: 314\tLoss: 0.176901\n",
      "Train Epoch: 315\tLoss: 0.176623\n",
      "Train Epoch: 316\tLoss: 0.176345\n",
      "Train Epoch: 317\tLoss: 0.176071\n",
      "Train Epoch: 318\tLoss: 0.175796\n",
      "Train Epoch: 319\tLoss: 0.175523\n",
      "Train Epoch: 320\tLoss: 0.175250\n",
      "Train Epoch: 321\tLoss: 0.174978\n",
      "Train Epoch: 322\tLoss: 0.174707\n",
      "Train Epoch: 323\tLoss: 0.174438\n",
      "Train Epoch: 324\tLoss: 0.174168\n",
      "Train Epoch: 325\tLoss: 0.173902\n",
      "Train Epoch: 326\tLoss: 0.173636\n",
      "Train Epoch: 327\tLoss: 0.173371\n",
      "Train Epoch: 328\tLoss: 0.173107\n",
      "Train Epoch: 329\tLoss: 0.172844\n",
      "Train Epoch: 330\tLoss: 0.172581\n",
      "Train Epoch: 331\tLoss: 0.172320\n",
      "Train Epoch: 332\tLoss: 0.172059\n",
      "Train Epoch: 333\tLoss: 0.171801\n",
      "Train Epoch: 334\tLoss: 0.171542\n",
      "Train Epoch: 335\tLoss: 0.171284\n",
      "Train Epoch: 336\tLoss: 0.171026\n",
      "Train Epoch: 337\tLoss: 0.170770\n",
      "Train Epoch: 338\tLoss: 0.170514\n",
      "Train Epoch: 339\tLoss: 0.170259\n",
      "Train Epoch: 340\tLoss: 0.170006\n",
      "Train Epoch: 341\tLoss: 0.169754\n",
      "Train Epoch: 342\tLoss: 0.169503\n",
      "Train Epoch: 343\tLoss: 0.169253\n",
      "Train Epoch: 344\tLoss: 0.169002\n",
      "Train Epoch: 345\tLoss: 0.168754\n",
      "Train Epoch: 346\tLoss: 0.168507\n",
      "Train Epoch: 347\tLoss: 0.168260\n",
      "Train Epoch: 348\tLoss: 0.168015\n",
      "Train Epoch: 349\tLoss: 0.167769\n",
      "Train Epoch: 350\tLoss: 0.167525\n",
      "Train Epoch: 351\tLoss: 0.167282\n",
      "Train Epoch: 352\tLoss: 0.167039\n",
      "Train Epoch: 353\tLoss: 0.166799\n",
      "Train Epoch: 354\tLoss: 0.166559\n",
      "Train Epoch: 355\tLoss: 0.166320\n",
      "Train Epoch: 356\tLoss: 0.166082\n",
      "Train Epoch: 357\tLoss: 0.165845\n",
      "Train Epoch: 358\tLoss: 0.165608\n",
      "Train Epoch: 359\tLoss: 0.165374\n",
      "Train Epoch: 360\tLoss: 0.165140\n",
      "Train Epoch: 361\tLoss: 0.164906\n",
      "Train Epoch: 362\tLoss: 0.164674\n",
      "Train Epoch: 363\tLoss: 0.164442\n",
      "Train Epoch: 364\tLoss: 0.164212\n",
      "Train Epoch: 365\tLoss: 0.163981\n",
      "Train Epoch: 366\tLoss: 0.163752\n",
      "Train Epoch: 367\tLoss: 0.163524\n",
      "Train Epoch: 368\tLoss: 0.163296\n",
      "Train Epoch: 369\tLoss: 0.163069\n",
      "Train Epoch: 370\tLoss: 0.162843\n",
      "Train Epoch: 371\tLoss: 0.162618\n",
      "Train Epoch: 372\tLoss: 0.162393\n",
      "Train Epoch: 373\tLoss: 0.162170\n",
      "Train Epoch: 374\tLoss: 0.161946\n",
      "Train Epoch: 375\tLoss: 0.161724\n",
      "Train Epoch: 376\tLoss: 0.161503\n",
      "Train Epoch: 377\tLoss: 0.161282\n",
      "Train Epoch: 378\tLoss: 0.161061\n",
      "Train Epoch: 379\tLoss: 0.160842\n",
      "Train Epoch: 380\tLoss: 0.160625\n",
      "Train Epoch: 381\tLoss: 0.160408\n",
      "Train Epoch: 382\tLoss: 0.160190\n",
      "Train Epoch: 383\tLoss: 0.159975\n",
      "Train Epoch: 384\tLoss: 0.159760\n",
      "Train Epoch: 385\tLoss: 0.159546\n",
      "Train Epoch: 386\tLoss: 0.159333\n",
      "Train Epoch: 387\tLoss: 0.159120\n",
      "Train Epoch: 388\tLoss: 0.158907\n",
      "Train Epoch: 389\tLoss: 0.158696\n",
      "Train Epoch: 390\tLoss: 0.158485\n",
      "Train Epoch: 391\tLoss: 0.158276\n",
      "Train Epoch: 392\tLoss: 0.158065\n",
      "Train Epoch: 393\tLoss: 0.157857\n",
      "Train Epoch: 394\tLoss: 0.157648\n",
      "Train Epoch: 395\tLoss: 0.157440\n",
      "Train Epoch: 396\tLoss: 0.157233\n",
      "Train Epoch: 397\tLoss: 0.157026\n",
      "Train Epoch: 398\tLoss: 0.156822\n",
      "Train Epoch: 399\tLoss: 0.156617\n",
      "Train Epoch: 400\tLoss: 0.156413\n",
      "Train Epoch: 401\tLoss: 0.156210\n",
      "Train Epoch: 402\tLoss: 0.156007\n",
      "Train Epoch: 403\tLoss: 0.155804\n",
      "Train Epoch: 404\tLoss: 0.155604\n",
      "Train Epoch: 405\tLoss: 0.155404\n",
      "Train Epoch: 406\tLoss: 0.155202\n",
      "Train Epoch: 407\tLoss: 0.155002\n",
      "Train Epoch: 408\tLoss: 0.154803\n",
      "Train Epoch: 409\tLoss: 0.154605\n",
      "Train Epoch: 410\tLoss: 0.154407\n",
      "Train Epoch: 411\tLoss: 0.154211\n",
      "Train Epoch: 412\tLoss: 0.154014\n",
      "Train Epoch: 413\tLoss: 0.153819\n",
      "Train Epoch: 414\tLoss: 0.153623\n",
      "Train Epoch: 415\tLoss: 0.153429\n",
      "Train Epoch: 416\tLoss: 0.153235\n",
      "Train Epoch: 417\tLoss: 0.153042\n",
      "Train Epoch: 418\tLoss: 0.152850\n",
      "Train Epoch: 419\tLoss: 0.152657\n",
      "Train Epoch: 420\tLoss: 0.152466\n",
      "Train Epoch: 421\tLoss: 0.152275\n",
      "Train Epoch: 422\tLoss: 0.152084\n",
      "Train Epoch: 423\tLoss: 0.151894\n",
      "Train Epoch: 424\tLoss: 0.151705\n",
      "Train Epoch: 425\tLoss: 0.151517\n",
      "Train Epoch: 426\tLoss: 0.151328\n",
      "Train Epoch: 427\tLoss: 0.151140\n",
      "Train Epoch: 428\tLoss: 0.150953\n",
      "Train Epoch: 429\tLoss: 0.150767\n",
      "Train Epoch: 430\tLoss: 0.150581\n",
      "Train Epoch: 431\tLoss: 0.150395\n",
      "Train Epoch: 432\tLoss: 0.150209\n",
      "Train Epoch: 433\tLoss: 0.150025\n",
      "Train Epoch: 434\tLoss: 0.149842\n",
      "Train Epoch: 435\tLoss: 0.149658\n",
      "Train Epoch: 436\tLoss: 0.149476\n",
      "Train Epoch: 437\tLoss: 0.149294\n",
      "Train Epoch: 438\tLoss: 0.149112\n",
      "Train Epoch: 439\tLoss: 0.148931\n",
      "Train Epoch: 440\tLoss: 0.148751\n",
      "Train Epoch: 441\tLoss: 0.148571\n",
      "Train Epoch: 442\tLoss: 0.148391\n",
      "Train Epoch: 443\tLoss: 0.148213\n",
      "Train Epoch: 444\tLoss: 0.148034\n",
      "Train Epoch: 445\tLoss: 0.147855\n",
      "Train Epoch: 446\tLoss: 0.147678\n",
      "Train Epoch: 447\tLoss: 0.147500\n",
      "Train Epoch: 448\tLoss: 0.147323\n",
      "Train Epoch: 449\tLoss: 0.147148\n",
      "Train Epoch: 450\tLoss: 0.146972\n",
      "Train Epoch: 451\tLoss: 0.146796\n",
      "Train Epoch: 452\tLoss: 0.146621\n",
      "Train Epoch: 453\tLoss: 0.146448\n",
      "Train Epoch: 454\tLoss: 0.146274\n",
      "Train Epoch: 455\tLoss: 0.146100\n",
      "Train Epoch: 456\tLoss: 0.145928\n",
      "Train Epoch: 457\tLoss: 0.145756\n",
      "Train Epoch: 458\tLoss: 0.145583\n",
      "Train Epoch: 459\tLoss: 0.145412\n",
      "Train Epoch: 460\tLoss: 0.145241\n",
      "Train Epoch: 461\tLoss: 0.145071\n",
      "Train Epoch: 462\tLoss: 0.144901\n",
      "Train Epoch: 463\tLoss: 0.144731\n",
      "Train Epoch: 464\tLoss: 0.144562\n",
      "Train Epoch: 465\tLoss: 0.144394\n",
      "Train Epoch: 466\tLoss: 0.144226\n",
      "Train Epoch: 467\tLoss: 0.144059\n",
      "Train Epoch: 468\tLoss: 0.143891\n",
      "Train Epoch: 469\tLoss: 0.143724\n",
      "Train Epoch: 470\tLoss: 0.143558\n",
      "Train Epoch: 471\tLoss: 0.143392\n",
      "Train Epoch: 472\tLoss: 0.143228\n",
      "Train Epoch: 473\tLoss: 0.143063\n",
      "Train Epoch: 474\tLoss: 0.142898\n",
      "Train Epoch: 475\tLoss: 0.142735\n",
      "Train Epoch: 476\tLoss: 0.142572\n",
      "Train Epoch: 477\tLoss: 0.142408\n",
      "Train Epoch: 478\tLoss: 0.142246\n",
      "Train Epoch: 479\tLoss: 0.142084\n",
      "Train Epoch: 480\tLoss: 0.141922\n",
      "Train Epoch: 481\tLoss: 0.141761\n",
      "Train Epoch: 482\tLoss: 0.141601\n",
      "Train Epoch: 483\tLoss: 0.141440\n",
      "Train Epoch: 484\tLoss: 0.141281\n",
      "Train Epoch: 485\tLoss: 0.141122\n",
      "Train Epoch: 486\tLoss: 0.140963\n",
      "Train Epoch: 487\tLoss: 0.140804\n",
      "Train Epoch: 488\tLoss: 0.140646\n",
      "Train Epoch: 489\tLoss: 0.140489\n",
      "Train Epoch: 490\tLoss: 0.140332\n",
      "Train Epoch: 491\tLoss: 0.140175\n",
      "Train Epoch: 492\tLoss: 0.140019\n",
      "Train Epoch: 493\tLoss: 0.139863\n",
      "Train Epoch: 494\tLoss: 0.139707\n",
      "Train Epoch: 495\tLoss: 0.139552\n",
      "Train Epoch: 496\tLoss: 0.139397\n",
      "Train Epoch: 497\tLoss: 0.139243\n",
      "Train Epoch: 498\tLoss: 0.139090\n",
      "Train Epoch: 499\tLoss: 0.138936\n",
      "Train Epoch: 500\tLoss: 0.138783\n",
      "Train Epoch: 501\tLoss: 0.138631\n",
      "Train Epoch: 502\tLoss: 0.138479\n",
      "Train Epoch: 503\tLoss: 0.138326\n",
      "Train Epoch: 504\tLoss: 0.138175\n",
      "Train Epoch: 505\tLoss: 0.138024\n",
      "Train Epoch: 506\tLoss: 0.137874\n",
      "Train Epoch: 507\tLoss: 0.137724\n",
      "Train Epoch: 508\tLoss: 0.137574\n",
      "Train Epoch: 509\tLoss: 0.137425\n",
      "Train Epoch: 510\tLoss: 0.137276\n",
      "Train Epoch: 511\tLoss: 0.137127\n",
      "Train Epoch: 512\tLoss: 0.136979\n",
      "Train Epoch: 513\tLoss: 0.136832\n",
      "Train Epoch: 514\tLoss: 0.136685\n",
      "Train Epoch: 515\tLoss: 0.136538\n",
      "Train Epoch: 516\tLoss: 0.136392\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 517\tLoss: 0.136246\n",
      "Train Epoch: 518\tLoss: 0.136100\n",
      "Train Epoch: 519\tLoss: 0.135954\n",
      "Train Epoch: 520\tLoss: 0.135810\n",
      "Train Epoch: 521\tLoss: 0.135665\n",
      "Train Epoch: 522\tLoss: 0.135519\n",
      "Train Epoch: 523\tLoss: 0.135376\n",
      "Train Epoch: 524\tLoss: 0.135231\n",
      "Train Epoch: 525\tLoss: 0.135088\n",
      "Train Epoch: 526\tLoss: 0.134945\n",
      "Train Epoch: 527\tLoss: 0.134803\n",
      "Train Epoch: 528\tLoss: 0.134660\n",
      "Train Epoch: 529\tLoss: 0.134518\n",
      "Train Epoch: 530\tLoss: 0.134376\n",
      "Train Epoch: 531\tLoss: 0.134234\n",
      "Train Epoch: 532\tLoss: 0.134094\n",
      "Train Epoch: 533\tLoss: 0.133953\n",
      "Train Epoch: 534\tLoss: 0.133812\n",
      "Train Epoch: 535\tLoss: 0.133673\n",
      "Train Epoch: 536\tLoss: 0.133533\n",
      "Train Epoch: 537\tLoss: 0.133395\n",
      "Train Epoch: 538\tLoss: 0.133256\n",
      "Train Epoch: 539\tLoss: 0.133118\n",
      "Train Epoch: 540\tLoss: 0.132979\n",
      "Train Epoch: 541\tLoss: 0.132842\n",
      "Train Epoch: 542\tLoss: 0.132705\n",
      "Train Epoch: 543\tLoss: 0.132568\n",
      "Train Epoch: 544\tLoss: 0.132431\n",
      "Train Epoch: 545\tLoss: 0.132295\n",
      "Train Epoch: 546\tLoss: 0.132158\n",
      "Train Epoch: 547\tLoss: 0.132023\n",
      "Train Epoch: 548\tLoss: 0.131887\n",
      "Train Epoch: 549\tLoss: 0.131752\n",
      "Train Epoch: 550\tLoss: 0.131617\n",
      "Train Epoch: 551\tLoss: 0.131483\n",
      "Train Epoch: 552\tLoss: 0.131349\n",
      "Train Epoch: 553\tLoss: 0.131215\n",
      "Train Epoch: 554\tLoss: 0.131081\n",
      "Train Epoch: 555\tLoss: 0.130949\n",
      "Train Epoch: 556\tLoss: 0.130816\n",
      "Train Epoch: 557\tLoss: 0.130684\n",
      "Train Epoch: 558\tLoss: 0.130553\n",
      "Train Epoch: 559\tLoss: 0.130422\n",
      "Train Epoch: 560\tLoss: 0.130291\n",
      "Train Epoch: 561\tLoss: 0.130160\n",
      "Train Epoch: 562\tLoss: 0.130030\n",
      "Train Epoch: 563\tLoss: 0.129900\n",
      "Train Epoch: 564\tLoss: 0.129770\n",
      "Train Epoch: 565\tLoss: 0.129641\n",
      "Train Epoch: 566\tLoss: 0.129513\n",
      "Train Epoch: 567\tLoss: 0.129384\n",
      "Train Epoch: 568\tLoss: 0.129256\n",
      "Train Epoch: 569\tLoss: 0.129128\n",
      "Train Epoch: 570\tLoss: 0.129000\n",
      "Train Epoch: 571\tLoss: 0.128871\n",
      "Train Epoch: 572\tLoss: 0.128744\n",
      "Train Epoch: 573\tLoss: 0.128617\n",
      "Train Epoch: 574\tLoss: 0.128490\n",
      "Train Epoch: 575\tLoss: 0.128365\n",
      "Train Epoch: 576\tLoss: 0.128238\n",
      "Train Epoch: 577\tLoss: 0.128112\n",
      "Train Epoch: 578\tLoss: 0.127987\n",
      "Train Epoch: 579\tLoss: 0.127862\n",
      "Train Epoch: 580\tLoss: 0.127737\n",
      "Train Epoch: 581\tLoss: 0.127612\n",
      "Train Epoch: 582\tLoss: 0.127488\n",
      "Train Epoch: 583\tLoss: 0.127364\n",
      "Train Epoch: 584\tLoss: 0.127240\n",
      "Train Epoch: 585\tLoss: 0.127117\n",
      "Train Epoch: 586\tLoss: 0.126994\n",
      "Train Epoch: 587\tLoss: 0.126871\n",
      "Train Epoch: 588\tLoss: 0.126748\n",
      "Train Epoch: 589\tLoss: 0.126626\n",
      "Train Epoch: 590\tLoss: 0.126504\n",
      "Train Epoch: 591\tLoss: 0.126383\n",
      "Train Epoch: 592\tLoss: 0.126261\n",
      "Train Epoch: 593\tLoss: 0.126140\n",
      "Train Epoch: 594\tLoss: 0.126018\n",
      "Train Epoch: 595\tLoss: 0.125898\n",
      "Train Epoch: 596\tLoss: 0.125778\n",
      "Train Epoch: 597\tLoss: 0.125658\n",
      "Train Epoch: 598\tLoss: 0.125538\n",
      "Train Epoch: 599\tLoss: 0.125419\n",
      "Train Epoch: 600\tLoss: 0.125299\n",
      "Train Epoch: 601\tLoss: 0.125180\n",
      "Train Epoch: 602\tLoss: 0.125061\n",
      "Train Epoch: 603\tLoss: 0.124942\n",
      "Train Epoch: 604\tLoss: 0.124824\n",
      "Train Epoch: 605\tLoss: 0.124706\n",
      "Train Epoch: 606\tLoss: 0.124588\n",
      "Train Epoch: 607\tLoss: 0.124470\n",
      "Train Epoch: 608\tLoss: 0.124353\n",
      "Train Epoch: 609\tLoss: 0.124236\n",
      "Train Epoch: 610\tLoss: 0.124120\n",
      "Train Epoch: 611\tLoss: 0.124003\n",
      "Train Epoch: 612\tLoss: 0.123886\n",
      "Train Epoch: 613\tLoss: 0.123770\n",
      "Train Epoch: 614\tLoss: 0.123655\n",
      "Train Epoch: 615\tLoss: 0.123539\n",
      "Train Epoch: 616\tLoss: 0.123423\n",
      "Train Epoch: 617\tLoss: 0.123308\n",
      "Train Epoch: 618\tLoss: 0.123194\n",
      "Train Epoch: 619\tLoss: 0.123079\n",
      "Train Epoch: 620\tLoss: 0.122965\n",
      "Train Epoch: 621\tLoss: 0.122851\n",
      "Train Epoch: 622\tLoss: 0.122738\n",
      "Train Epoch: 623\tLoss: 0.122625\n",
      "Train Epoch: 624\tLoss: 0.122512\n",
      "Train Epoch: 625\tLoss: 0.122399\n",
      "Train Epoch: 626\tLoss: 0.122286\n",
      "Train Epoch: 627\tLoss: 0.122174\n",
      "Train Epoch: 628\tLoss: 0.122061\n",
      "Train Epoch: 629\tLoss: 0.121950\n",
      "Train Epoch: 630\tLoss: 0.121838\n",
      "Train Epoch: 631\tLoss: 0.121727\n",
      "Train Epoch: 632\tLoss: 0.121616\n",
      "Train Epoch: 633\tLoss: 0.121505\n",
      "Train Epoch: 634\tLoss: 0.121394\n",
      "Train Epoch: 635\tLoss: 0.121283\n",
      "Train Epoch: 636\tLoss: 0.121173\n",
      "Train Epoch: 637\tLoss: 0.121064\n",
      "Train Epoch: 638\tLoss: 0.120954\n",
      "Train Epoch: 639\tLoss: 0.120844\n",
      "Train Epoch: 640\tLoss: 0.120735\n",
      "Train Epoch: 641\tLoss: 0.120626\n",
      "Train Epoch: 642\tLoss: 0.120516\n",
      "Train Epoch: 643\tLoss: 0.120408\n",
      "Train Epoch: 644\tLoss: 0.120299\n",
      "Train Epoch: 645\tLoss: 0.120190\n",
      "Train Epoch: 646\tLoss: 0.120083\n",
      "Train Epoch: 647\tLoss: 0.119974\n",
      "Train Epoch: 648\tLoss: 0.119867\n",
      "Train Epoch: 649\tLoss: 0.119759\n",
      "Train Epoch: 650\tLoss: 0.119651\n",
      "Train Epoch: 651\tLoss: 0.119544\n",
      "Train Epoch: 652\tLoss: 0.119437\n",
      "Train Epoch: 653\tLoss: 0.119331\n",
      "Train Epoch: 654\tLoss: 0.119224\n",
      "Train Epoch: 655\tLoss: 0.119118\n",
      "Train Epoch: 656\tLoss: 0.119011\n",
      "Train Epoch: 657\tLoss: 0.118905\n",
      "Train Epoch: 658\tLoss: 0.118800\n",
      "Train Epoch: 659\tLoss: 0.118694\n",
      "Train Epoch: 660\tLoss: 0.118588\n",
      "Train Epoch: 661\tLoss: 0.118483\n",
      "Train Epoch: 662\tLoss: 0.118377\n",
      "Train Epoch: 663\tLoss: 0.118273\n",
      "Train Epoch: 664\tLoss: 0.118168\n",
      "Train Epoch: 665\tLoss: 0.118064\n",
      "Train Epoch: 666\tLoss: 0.117960\n",
      "Train Epoch: 667\tLoss: 0.117855\n",
      "Train Epoch: 668\tLoss: 0.117752\n",
      "Train Epoch: 669\tLoss: 0.117648\n",
      "Train Epoch: 670\tLoss: 0.117545\n",
      "Train Epoch: 671\tLoss: 0.117442\n",
      "Train Epoch: 672\tLoss: 0.117338\n",
      "Train Epoch: 673\tLoss: 0.117235\n",
      "Train Epoch: 674\tLoss: 0.117133\n",
      "Train Epoch: 675\tLoss: 0.117030\n",
      "Train Epoch: 676\tLoss: 0.116928\n",
      "Train Epoch: 677\tLoss: 0.116826\n",
      "Train Epoch: 678\tLoss: 0.116725\n",
      "Train Epoch: 679\tLoss: 0.116622\n",
      "Train Epoch: 680\tLoss: 0.116521\n",
      "Train Epoch: 681\tLoss: 0.116419\n",
      "Train Epoch: 682\tLoss: 0.116319\n",
      "Train Epoch: 683\tLoss: 0.116219\n",
      "Train Epoch: 684\tLoss: 0.116118\n",
      "Train Epoch: 685\tLoss: 0.116017\n",
      "Train Epoch: 686\tLoss: 0.115916\n",
      "Train Epoch: 687\tLoss: 0.115817\n",
      "Train Epoch: 688\tLoss: 0.115717\n",
      "Train Epoch: 689\tLoss: 0.115617\n",
      "Train Epoch: 690\tLoss: 0.115518\n",
      "Train Epoch: 691\tLoss: 0.115418\n",
      "Train Epoch: 692\tLoss: 0.115319\n",
      "Train Epoch: 693\tLoss: 0.115221\n",
      "Train Epoch: 694\tLoss: 0.115122\n",
      "Train Epoch: 695\tLoss: 0.115023\n",
      "Train Epoch: 696\tLoss: 0.114925\n",
      "Train Epoch: 697\tLoss: 0.114827\n",
      "Train Epoch: 698\tLoss: 0.114729\n",
      "Train Epoch: 699\tLoss: 0.114630\n",
      "Train Epoch: 700\tLoss: 0.114533\n",
      "Train Epoch: 701\tLoss: 0.114435\n",
      "Train Epoch: 702\tLoss: 0.114337\n",
      "Train Epoch: 703\tLoss: 0.114240\n",
      "Train Epoch: 704\tLoss: 0.114143\n",
      "Train Epoch: 705\tLoss: 0.114045\n",
      "Train Epoch: 706\tLoss: 0.113949\n",
      "Train Epoch: 707\tLoss: 0.113852\n",
      "Train Epoch: 708\tLoss: 0.113755\n",
      "Train Epoch: 709\tLoss: 0.113658\n",
      "Train Epoch: 710\tLoss: 0.113562\n",
      "Train Epoch: 711\tLoss: 0.113466\n",
      "Train Epoch: 712\tLoss: 0.113370\n",
      "Train Epoch: 713\tLoss: 0.113275\n",
      "Train Epoch: 714\tLoss: 0.113179\n",
      "Train Epoch: 715\tLoss: 0.113083\n",
      "Train Epoch: 716\tLoss: 0.112988\n",
      "Train Epoch: 717\tLoss: 0.112893\n",
      "Train Epoch: 718\tLoss: 0.112797\n",
      "Train Epoch: 719\tLoss: 0.112702\n",
      "Train Epoch: 720\tLoss: 0.112608\n",
      "Train Epoch: 721\tLoss: 0.112513\n",
      "Train Epoch: 722\tLoss: 0.112418\n",
      "Train Epoch: 723\tLoss: 0.112324\n",
      "Train Epoch: 724\tLoss: 0.112230\n",
      "Train Epoch: 725\tLoss: 0.112136\n",
      "Train Epoch: 726\tLoss: 0.112042\n",
      "Train Epoch: 727\tLoss: 0.111948\n",
      "Train Epoch: 728\tLoss: 0.111855\n",
      "Train Epoch: 729\tLoss: 0.111762\n",
      "Train Epoch: 730\tLoss: 0.111669\n",
      "Train Epoch: 731\tLoss: 0.111576\n",
      "Train Epoch: 732\tLoss: 0.111483\n",
      "Train Epoch: 733\tLoss: 0.111391\n",
      "Train Epoch: 734\tLoss: 0.111299\n",
      "Train Epoch: 735\tLoss: 0.111207\n",
      "Train Epoch: 736\tLoss: 0.111115\n",
      "Train Epoch: 737\tLoss: 0.111023\n",
      "Train Epoch: 738\tLoss: 0.110932\n",
      "Train Epoch: 739\tLoss: 0.110840\n",
      "Train Epoch: 740\tLoss: 0.110749\n",
      "Train Epoch: 741\tLoss: 0.110658\n",
      "Train Epoch: 742\tLoss: 0.110567\n",
      "Train Epoch: 743\tLoss: 0.110476\n",
      "Train Epoch: 744\tLoss: 0.110385\n",
      "Train Epoch: 745\tLoss: 0.110295\n",
      "Train Epoch: 746\tLoss: 0.110205\n",
      "Train Epoch: 747\tLoss: 0.110114\n",
      "Train Epoch: 748\tLoss: 0.110024\n",
      "Train Epoch: 749\tLoss: 0.109935\n",
      "Train Epoch: 750\tLoss: 0.109845\n",
      "Train Epoch: 751\tLoss: 0.109755\n",
      "Train Epoch: 752\tLoss: 0.109665\n",
      "Train Epoch: 753\tLoss: 0.109575\n",
      "Train Epoch: 754\tLoss: 0.109487\n",
      "Train Epoch: 755\tLoss: 0.109397\n",
      "Train Epoch: 756\tLoss: 0.109308\n",
      "Train Epoch: 757\tLoss: 0.109220\n",
      "Train Epoch: 758\tLoss: 0.109131\n",
      "Train Epoch: 759\tLoss: 0.109043\n",
      "Train Epoch: 760\tLoss: 0.108955\n",
      "Train Epoch: 761\tLoss: 0.108867\n",
      "Train Epoch: 762\tLoss: 0.108779\n",
      "Train Epoch: 763\tLoss: 0.108691\n",
      "Train Epoch: 764\tLoss: 0.108604\n",
      "Train Epoch: 765\tLoss: 0.108516\n",
      "Train Epoch: 766\tLoss: 0.108430\n",
      "Train Epoch: 767\tLoss: 0.108343\n",
      "Train Epoch: 768\tLoss: 0.108255\n",
      "Train Epoch: 769\tLoss: 0.108168\n",
      "Train Epoch: 770\tLoss: 0.108081\n",
      "Train Epoch: 771\tLoss: 0.107995\n",
      "Train Epoch: 772\tLoss: 0.107909\n",
      "Train Epoch: 773\tLoss: 0.107822\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 774\tLoss: 0.107736\n",
      "Train Epoch: 775\tLoss: 0.107650\n",
      "Train Epoch: 776\tLoss: 0.107565\n",
      "Train Epoch: 777\tLoss: 0.107479\n",
      "Train Epoch: 778\tLoss: 0.107394\n",
      "Train Epoch: 779\tLoss: 0.107308\n",
      "Train Epoch: 780\tLoss: 0.107223\n",
      "Train Epoch: 781\tLoss: 0.107138\n",
      "Train Epoch: 782\tLoss: 0.107053\n",
      "Train Epoch: 783\tLoss: 0.106968\n",
      "Train Epoch: 784\tLoss: 0.106884\n",
      "Train Epoch: 785\tLoss: 0.106800\n",
      "Train Epoch: 786\tLoss: 0.106715\n",
      "Train Epoch: 787\tLoss: 0.106631\n",
      "Train Epoch: 788\tLoss: 0.106547\n",
      "Train Epoch: 789\tLoss: 0.106463\n",
      "Train Epoch: 790\tLoss: 0.106378\n",
      "Train Epoch: 791\tLoss: 0.106295\n",
      "Train Epoch: 792\tLoss: 0.106211\n",
      "Train Epoch: 793\tLoss: 0.106128\n",
      "Train Epoch: 794\tLoss: 0.106044\n",
      "Train Epoch: 795\tLoss: 0.105961\n",
      "Train Epoch: 796\tLoss: 0.105877\n",
      "Train Epoch: 797\tLoss: 0.105794\n",
      "Train Epoch: 798\tLoss: 0.105712\n",
      "Train Epoch: 799\tLoss: 0.105629\n",
      "Train Epoch: 800\tLoss: 0.105546\n",
      "Train Epoch: 801\tLoss: 0.105464\n",
      "Train Epoch: 802\tLoss: 0.105381\n",
      "Train Epoch: 803\tLoss: 0.105300\n",
      "Train Epoch: 804\tLoss: 0.105217\n",
      "Train Epoch: 805\tLoss: 0.105136\n",
      "Train Epoch: 806\tLoss: 0.105054\n",
      "Train Epoch: 807\tLoss: 0.104973\n",
      "Train Epoch: 808\tLoss: 0.104891\n",
      "Train Epoch: 809\tLoss: 0.104809\n",
      "Train Epoch: 810\tLoss: 0.104728\n",
      "Train Epoch: 811\tLoss: 0.104647\n",
      "Train Epoch: 812\tLoss: 0.104566\n",
      "Train Epoch: 813\tLoss: 0.104485\n",
      "Train Epoch: 814\tLoss: 0.104405\n",
      "Train Epoch: 815\tLoss: 0.104323\n",
      "Train Epoch: 816\tLoss: 0.104243\n",
      "Train Epoch: 817\tLoss: 0.104163\n",
      "Train Epoch: 818\tLoss: 0.104083\n",
      "Train Epoch: 819\tLoss: 0.104003\n",
      "Train Epoch: 820\tLoss: 0.103923\n",
      "Train Epoch: 821\tLoss: 0.103843\n",
      "Train Epoch: 822\tLoss: 0.103764\n",
      "Train Epoch: 823\tLoss: 0.103684\n",
      "Train Epoch: 824\tLoss: 0.103605\n",
      "Train Epoch: 825\tLoss: 0.103525\n",
      "Train Epoch: 826\tLoss: 0.103446\n",
      "Train Epoch: 827\tLoss: 0.103367\n",
      "Train Epoch: 828\tLoss: 0.103288\n",
      "Train Epoch: 829\tLoss: 0.103209\n",
      "Train Epoch: 830\tLoss: 0.103130\n",
      "Train Epoch: 831\tLoss: 0.103052\n",
      "Train Epoch: 832\tLoss: 0.102974\n",
      "Train Epoch: 833\tLoss: 0.102895\n",
      "Train Epoch: 834\tLoss: 0.102818\n",
      "Train Epoch: 835\tLoss: 0.102739\n",
      "Train Epoch: 836\tLoss: 0.102661\n",
      "Train Epoch: 837\tLoss: 0.102584\n",
      "Train Epoch: 838\tLoss: 0.102506\n",
      "Train Epoch: 839\tLoss: 0.102429\n",
      "Train Epoch: 840\tLoss: 0.102352\n",
      "Train Epoch: 841\tLoss: 0.102274\n",
      "Train Epoch: 842\tLoss: 0.102197\n",
      "Train Epoch: 843\tLoss: 0.102120\n",
      "Train Epoch: 844\tLoss: 0.102043\n",
      "Train Epoch: 845\tLoss: 0.101967\n",
      "Train Epoch: 846\tLoss: 0.101890\n",
      "Train Epoch: 847\tLoss: 0.101813\n",
      "Train Epoch: 848\tLoss: 0.101737\n",
      "Train Epoch: 849\tLoss: 0.101661\n",
      "Train Epoch: 850\tLoss: 0.101584\n",
      "Train Epoch: 851\tLoss: 0.101508\n",
      "Train Epoch: 852\tLoss: 0.101433\n",
      "Train Epoch: 853\tLoss: 0.101357\n",
      "Train Epoch: 854\tLoss: 0.101282\n",
      "Train Epoch: 855\tLoss: 0.101206\n",
      "Train Epoch: 856\tLoss: 0.101130\n",
      "Train Epoch: 857\tLoss: 0.101055\n",
      "Train Epoch: 858\tLoss: 0.100980\n",
      "Train Epoch: 859\tLoss: 0.100905\n",
      "Train Epoch: 860\tLoss: 0.100829\n",
      "Train Epoch: 861\tLoss: 0.100755\n",
      "Train Epoch: 862\tLoss: 0.100680\n",
      "Train Epoch: 863\tLoss: 0.100605\n",
      "Train Epoch: 864\tLoss: 0.100530\n",
      "Train Epoch: 865\tLoss: 0.100456\n",
      "Train Epoch: 866\tLoss: 0.100381\n",
      "Train Epoch: 867\tLoss: 0.100307\n",
      "Train Epoch: 868\tLoss: 0.100233\n",
      "Train Epoch: 869\tLoss: 0.100159\n",
      "Train Epoch: 870\tLoss: 0.100084\n",
      "Train Epoch: 871\tLoss: 0.100010\n",
      "Train Epoch: 872\tLoss: 0.099936\n",
      "Train Epoch: 873\tLoss: 0.099862\n",
      "Train Epoch: 874\tLoss: 0.099789\n",
      "Train Epoch: 875\tLoss: 0.099715\n",
      "Train Epoch: 876\tLoss: 0.099642\n",
      "Train Epoch: 877\tLoss: 0.099568\n",
      "Train Epoch: 878\tLoss: 0.099494\n",
      "Train Epoch: 879\tLoss: 0.099421\n",
      "Train Epoch: 880\tLoss: 0.099348\n",
      "Train Epoch: 881\tLoss: 0.099275\n",
      "Train Epoch: 882\tLoss: 0.099202\n",
      "Train Epoch: 883\tLoss: 0.099129\n",
      "Train Epoch: 884\tLoss: 0.099056\n",
      "Train Epoch: 885\tLoss: 0.098984\n",
      "Train Epoch: 886\tLoss: 0.098911\n",
      "Train Epoch: 887\tLoss: 0.098839\n",
      "Train Epoch: 888\tLoss: 0.098766\n",
      "Train Epoch: 889\tLoss: 0.098694\n",
      "Train Epoch: 890\tLoss: 0.098622\n",
      "Train Epoch: 891\tLoss: 0.098550\n",
      "Train Epoch: 892\tLoss: 0.098478\n",
      "Train Epoch: 893\tLoss: 0.098406\n",
      "Train Epoch: 894\tLoss: 0.098335\n",
      "Train Epoch: 895\tLoss: 0.098263\n",
      "Train Epoch: 896\tLoss: 0.098192\n",
      "Train Epoch: 897\tLoss: 0.098121\n",
      "Train Epoch: 898\tLoss: 0.098050\n",
      "Train Epoch: 899\tLoss: 0.097979\n",
      "Train Epoch: 900\tLoss: 0.097908\n",
      "Train Epoch: 901\tLoss: 0.097837\n",
      "Train Epoch: 902\tLoss: 0.097766\n",
      "Train Epoch: 903\tLoss: 0.097696\n",
      "Train Epoch: 904\tLoss: 0.097625\n",
      "Train Epoch: 905\tLoss: 0.097554\n",
      "Train Epoch: 906\tLoss: 0.097484\n",
      "Train Epoch: 907\tLoss: 0.097413\n",
      "Train Epoch: 908\tLoss: 0.097343\n",
      "Train Epoch: 909\tLoss: 0.097273\n",
      "Train Epoch: 910\tLoss: 0.097203\n",
      "Train Epoch: 911\tLoss: 0.097133\n",
      "Train Epoch: 912\tLoss: 0.097063\n",
      "Train Epoch: 913\tLoss: 0.096994\n",
      "Train Epoch: 914\tLoss: 0.096924\n",
      "Train Epoch: 915\tLoss: 0.096855\n",
      "Train Epoch: 916\tLoss: 0.096785\n",
      "Train Epoch: 917\tLoss: 0.096715\n",
      "Train Epoch: 918\tLoss: 0.096646\n",
      "Train Epoch: 919\tLoss: 0.096577\n",
      "Train Epoch: 920\tLoss: 0.096507\n",
      "Train Epoch: 921\tLoss: 0.096438\n",
      "Train Epoch: 922\tLoss: 0.096369\n",
      "Train Epoch: 923\tLoss: 0.096300\n",
      "Train Epoch: 924\tLoss: 0.096231\n",
      "Train Epoch: 925\tLoss: 0.096163\n",
      "Train Epoch: 926\tLoss: 0.096094\n",
      "Train Epoch: 927\tLoss: 0.096025\n",
      "Train Epoch: 928\tLoss: 0.095957\n",
      "Train Epoch: 929\tLoss: 0.095889\n",
      "Train Epoch: 930\tLoss: 0.095820\n",
      "Train Epoch: 931\tLoss: 0.095753\n",
      "Train Epoch: 932\tLoss: 0.095685\n",
      "Train Epoch: 933\tLoss: 0.095617\n",
      "Train Epoch: 934\tLoss: 0.095549\n",
      "Train Epoch: 935\tLoss: 0.095481\n",
      "Train Epoch: 936\tLoss: 0.095413\n",
      "Train Epoch: 937\tLoss: 0.095346\n",
      "Train Epoch: 938\tLoss: 0.095279\n",
      "Train Epoch: 939\tLoss: 0.095211\n",
      "Train Epoch: 940\tLoss: 0.095144\n",
      "Train Epoch: 941\tLoss: 0.095076\n",
      "Train Epoch: 942\tLoss: 0.095009\n",
      "Train Epoch: 943\tLoss: 0.094943\n",
      "Train Epoch: 944\tLoss: 0.094875\n",
      "Train Epoch: 945\tLoss: 0.094809\n",
      "Train Epoch: 946\tLoss: 0.094742\n",
      "Train Epoch: 947\tLoss: 0.094675\n",
      "Train Epoch: 948\tLoss: 0.094608\n",
      "Train Epoch: 949\tLoss: 0.094542\n",
      "Train Epoch: 950\tLoss: 0.094475\n",
      "Train Epoch: 951\tLoss: 0.094409\n",
      "Train Epoch: 952\tLoss: 0.094342\n",
      "Train Epoch: 953\tLoss: 0.094276\n",
      "Train Epoch: 954\tLoss: 0.094210\n",
      "Train Epoch: 955\tLoss: 0.094144\n",
      "Train Epoch: 956\tLoss: 0.094078\n",
      "Train Epoch: 957\tLoss: 0.094012\n",
      "Train Epoch: 958\tLoss: 0.093946\n",
      "Train Epoch: 959\tLoss: 0.093880\n",
      "Train Epoch: 960\tLoss: 0.093814\n",
      "Train Epoch: 961\tLoss: 0.093748\n",
      "Train Epoch: 962\tLoss: 0.093683\n",
      "Train Epoch: 963\tLoss: 0.093618\n",
      "Train Epoch: 964\tLoss: 0.093553\n",
      "Train Epoch: 965\tLoss: 0.093488\n",
      "Train Epoch: 966\tLoss: 0.093422\n",
      "Train Epoch: 967\tLoss: 0.093357\n",
      "Train Epoch: 968\tLoss: 0.093292\n",
      "Train Epoch: 969\tLoss: 0.093227\n",
      "Train Epoch: 970\tLoss: 0.093163\n",
      "Train Epoch: 971\tLoss: 0.093097\n",
      "Train Epoch: 972\tLoss: 0.093033\n",
      "Train Epoch: 973\tLoss: 0.092968\n",
      "Train Epoch: 974\tLoss: 0.092904\n",
      "Train Epoch: 975\tLoss: 0.092839\n",
      "Train Epoch: 976\tLoss: 0.092774\n",
      "Train Epoch: 977\tLoss: 0.092710\n",
      "Train Epoch: 978\tLoss: 0.092646\n",
      "Train Epoch: 979\tLoss: 0.092581\n",
      "Train Epoch: 980\tLoss: 0.092518\n",
      "Train Epoch: 981\tLoss: 0.092453\n",
      "Train Epoch: 982\tLoss: 0.092390\n",
      "Train Epoch: 983\tLoss: 0.092326\n",
      "Train Epoch: 984\tLoss: 0.092262\n",
      "Train Epoch: 985\tLoss: 0.092198\n",
      "Train Epoch: 986\tLoss: 0.092134\n",
      "Train Epoch: 987\tLoss: 0.092070\n",
      "Train Epoch: 988\tLoss: 0.092007\n",
      "Train Epoch: 989\tLoss: 0.091943\n",
      "Train Epoch: 990\tLoss: 0.091880\n",
      "Train Epoch: 991\tLoss: 0.091816\n",
      "Train Epoch: 992\tLoss: 0.091753\n",
      "Train Epoch: 993\tLoss: 0.091691\n",
      "Train Epoch: 994\tLoss: 0.091628\n",
      "Train Epoch: 995\tLoss: 0.091564\n",
      "Train Epoch: 996\tLoss: 0.091502\n",
      "Train Epoch: 997\tLoss: 0.091439\n",
      "Train Epoch: 998\tLoss: 0.091376\n",
      "Train Epoch: 999\tLoss: 0.091313\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1000):\n",
    "    train(model, X_train, y_train, optimizer, criterion, epoch, 'print')\n",
    "#    test(model, X_test, y_test, criterion, 'print')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.1267\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test(model, X_test, y_test, criterion, 'print')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
